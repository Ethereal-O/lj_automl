"""
Alpha Scoring Calculator
ç”¨äºè®¡ç®—å·²è®¡ç®—å› å­å€¼ä¹‹é—´çš„ICï¼ˆä¿¡æ¯ç³»æ•°ï¼‰

æ­¤æ¨¡å—ä¸è´Ÿè´£å› å­è®¡ç®—ï¼Œåªä»ç¼“å­˜ä¸­è¯»å–é¢„è®¡ç®—çš„å› å­å€¼æ¥è®¡ç®—ICã€‚
å› å­å€¼ç”±external_compute_factoré¢„å…ˆè®¡ç®—å¹¶å­˜å‚¨ã€‚
"""

import pandas as pd
import numpy as np
from typing import Optional, Dict, List, Tuple
import sys
import hashlib
import pickle
from pathlib import Path
import os
import time
from datetime import datetime

# å¯¼å…¥æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.model_selection import train_test_split
    import lightgbm as lgb
    from scipy.stats import spearmanr
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: sklearn/lightgbm not available, LGB evaluation will be disabled", file=sys.stderr)

# é…ç½®
FACTOR_CACHE_DIR = "factor_cache"
TARGET_CACHE_FILE = "target_values.pkl"

# é¢„æµ‹ç›®æ ‡æ•°æ®é…ç½® - ä¿®æ”¹ä¸ºæ–°çš„CSVæ ¼å¼
FACTOR_DATA_ROOT_DIR = os.getenv('FACTOR_DATA_ROOT_DIR', "/dfs/dataset/10-1732512661487/data/StockLabel_adj_lnret")
INTERVAL_CONFIG = os.getenv('INTERVAL_CONFIG', "1dper1d")

# é¢„æµ‹ç›®æ ‡åˆ—åé…ç½®
TARGET_COLUMN = os.getenv('TARGET_COLUMN', 'yhat_raw_lnRet_t2ov_1d')  # æŒ‡å®šä½¿ç”¨çš„é¢„æµ‹ç›®æ ‡åˆ—å

# èšåˆæ”¶ç›Šç‡ç¼“å­˜ç›®å½•
RETURN_CACHE_DIR = "return_cache"


class FactorCache:
    """å› å­å€¼ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = FACTOR_CACHE_DIR):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_cache_key(self, expr: str) -> str:
        """ç”Ÿæˆè¡¨è¾¾å¼ç¼“å­˜é”®"""
        return hashlib.md5(expr.encode()).hexdigest()

    def save_factor(self, expr: str, values: np.ndarray, dates: pd.Index, symbols: pd.Index):
        """ä¿å­˜å› å­å€¼åˆ°ç¼“å­˜"""
        cache_key = self._get_cache_key(expr)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        data = {
            'expression': expr,
            'values': values,
            'dates': dates,
            'symbols': symbols
        }

        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)

    def load_factor(self, expr: str) -> Optional[Dict]:
        """ä»ç¼“å­˜åŠ è½½å› å­å€¼"""
        cache_key = self._get_cache_key(expr)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def has_factor(self, expr: str) -> bool:
        """æ£€æŸ¥å› å­æ˜¯å¦å·²ç¼“å­˜"""
        return self.load_factor(expr) is not None


class TargetManager:
    """ç›®æ ‡å€¼ç®¡ç†å™¨"""

    def __init__(self, target_file: str = TARGET_CACHE_FILE):
        self.target_file = Path(target_file)

    def save_target(self, values: np.ndarray, dates: pd.Index, symbols: pd.Index):
        """ä¿å­˜ç›®æ ‡å€¼"""
        data = {
            'values': values,
            'dates': dates,
            'symbols': symbols
        }

        with open(self.target_file, 'wb') as f:
            pickle.dump(data, f)

    def load_target(self) -> Optional[Dict]:
        """åŠ è½½ç›®æ ‡å€¼"""
        if self.target_file.exists():
            with open(self.target_file, 'rb') as f:
                return pickle.load(f)
        return None


# å…¨å±€å®ä¾‹
factor_cache = FactorCache()
target_manager = TargetManager()


class ReturnCache:
    """èšåˆæ”¶ç›Šç‡ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = RETURN_CACHE_DIR):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_cache_key(self, interval_config: str, start_date: str, end_date: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_str = f"{interval_config}_{start_date}_{end_date}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def save_returns(self, interval_config: str, start_date: str, end_date: str,
                    values: np.ndarray, dates: pd.Index, symbols: pd.Index):
        """ä¿å­˜èšåˆæ”¶ç›Šç‡"""
        cache_key = self._get_cache_key(interval_config, start_date, end_date)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        data = {
            'interval_config': interval_config,
            'start_date': start_date,
            'end_date': end_date,
            'values': values,
            'dates': dates,
            'symbols': symbols
        }

        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)

    def load_returns(self, interval_config: str, start_date: str, end_date: str) -> Optional[Dict]:
        """åŠ è½½èšåˆæ”¶ç›Šç‡"""
        cache_key = self._get_cache_key(interval_config, start_date, end_date)
        cache_file = self.cache_dir / f"{cache_key}.pkl"

        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None


# å…¨å±€å®ä¾‹
return_cache = ReturnCache()


def parse_interval_config(interval_config: str) -> Tuple[str, str]:
    """
    è§£æintervalé…ç½®ï¼Œè¿”å›(å› å­è®¡ç®—é—´éš”, é¢„æµ‹æ—¶é—´è·¨åº¦)

    Args:
        interval_config: å¦‚ "30per30", "5per5", "1dper1d", "10per30"ç­‰
                         aå¯ä»¥æ˜¯æ•°å­—(5,10,30)æˆ–"1d"ï¼Œbå†³å®šæ”¶ç›Šç‡èšåˆ

    Returns:
        (factor_interval, prediction_period) - factor_intervalå¯ä»¥æ˜¯æ•°å­—å­—ç¬¦ä¸²æˆ–"1d"
    """
    if 'per' not in interval_config:
        # é»˜è®¤é…ç½®
        return "30", "30"  # 30åˆ†é’Ÿé—´éš”ï¼Œé¢„æµ‹30åˆ†é’Ÿ

    parts = interval_config.split('per')
    if len(parts) != 2:
        return "30", "30"

    factor_interval = parts[0]  # ä¿æŒä¸ºå­—ç¬¦ä¸²ï¼Œæ”¯æŒ"1d"ç­‰æ ¼å¼
    prediction_period = parts[1]

    return factor_interval, prediction_period


def aggregate_minute_returns_for_interval(df: pd.DataFrame, factor_minute: int,
                                        prediction_period: str, date_str: str) -> pd.DataFrame:
    """
    ä»å› å­è®¡ç®—æ—¶åˆ»å¼€å§‹ï¼Œèšåˆæœªæ¥æŒ‡å®šæ—¶é—´æ®µçš„åˆ†é’Ÿæ”¶ç›Šç‡
    æ™ºèƒ½å¤„ç†æ•°æ®ä¸è¶³çš„æƒ…å†µï¼ŒåŠ¨æ€è°ƒæ•´é¢„æµ‹å‘¨æœŸ

    Args:
        df: åŒ…å«åˆ†é’Ÿæ•°æ®çš„DataFrame (symbol, minuteCode, label)
        factor_minute: å› å­è®¡ç®—çš„åˆ†é’Ÿæ—¶åˆ»
        prediction_period: é¢„æµ‹æ—¶é—´è·¨åº¦ ("30", "5", "1d"ç­‰)
        date_str: æ—¥æœŸå­—ç¬¦ä¸²

    Returns:
        èšåˆåçš„æ”¶ç›Šç‡DataFrame
    """
    try:
        # è§£æé¢„æµ‹æ—¶é—´è·¨åº¦
        if prediction_period.endswith('d'):
            # æ—¥çº§åˆ«é¢„æµ‹ï¼šæš‚æ—¶è¿”å›ç©ºDataFrame
            # TODO: å®ç°è·¨å¤©æ•°æ®è¯»å–
            print(f"  Note: Day-level prediction requires cross-day data (not implemented yet)", file=sys.stderr)
            return pd.DataFrame()

        else:
            # åˆ†é’Ÿçº§åˆ«é¢„æµ‹
            requested_minutes = int(prediction_period)

            # è®¡ç®—ç†è®ºä¸Šçš„æœªæ¥åˆ†é’Ÿ
            theoretical_future_minutes = list(range(factor_minute + 1, factor_minute + 1 + requested_minutes))

            # è·å–æ•°æ®ä¸­å®é™…å¯ç”¨çš„åˆ†é’Ÿ
            available_minutes = sorted(df['minuteCode'].unique())
            available_minutes = [m for m in available_minutes if m > factor_minute]

            if not available_minutes:
                # æ²¡æœ‰æœªæ¥æ•°æ®
                print(f"  Warning: No future data available after minute {factor_minute}", file=sys.stderr)
                return pd.DataFrame()

            # æ‰¾åˆ°é‡å çš„åˆ†é’Ÿï¼ˆå®é™…å¯ç”¨ä¸”åœ¨ç†è®ºèŒƒå›´å†…ï¼‰
            max_available_minute = max(available_minutes)
            max_theoretical_minute = factor_minute + requested_minutes

            # å®é™…å¯ç”¨çš„æœªæ¥åˆ†é’Ÿ
            actual_future_minutes = [m for m in theoretical_future_minutes if m in available_minutes]

            if not actual_future_minutes:
                print(f"  Warning: No overlapping future minutes found for factor_minute {factor_minute}", file=sys.stderr)
                return pd.DataFrame()

            # å¦‚æœå¯ç”¨æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®
            if len(actual_future_minutes) < requested_minutes:
                shortage = requested_minutes - len(actual_future_minutes)
                print(f"  Note: Using {len(actual_future_minutes)} minutes instead of requested {requested_minutes} (shortage: {shortage})", file=sys.stderr)

            future_minutes = actual_future_minutes

        if not future_minutes:
            return pd.DataFrame()

        # èšåˆæœªæ¥åˆ†é’Ÿçš„æ”¶ç›Šç‡
        # æ–¹æ³•ï¼šè¿ä¹˜ (1 + r1) * (1 + r2) * ... - 1
        future_returns = []
        for _, row in df.iterrows():
            stock_code = row['symbol']
            stock_minute_returns = []

            # æ”¶é›†è¯¥è‚¡ç¥¨åœ¨æœªæ¥æ—¶é—´æ®µçš„æ‰€æœ‰åˆ†é’Ÿæ”¶ç›Šç‡
            for minute in future_minutes:
                minute_data = df[(df['symbol'] == stock_code) & (df['minuteCode'] == minute)]
                if not minute_data.empty:
                    stock_minute_returns.append(minute_data['label'].iloc[0])
                else:
                    # å¦‚æœç¼ºå°‘åˆ†é’Ÿæ•°æ®ï¼Œç”¨0å¡«å……
                    stock_minute_returns.append(0.0)

            # è®¡ç®—ç´¯ç§¯æ”¶ç›Šç‡
            if stock_minute_returns:
                cumulative_return = 1.0
                for ret in stock_minute_returns:
                    cumulative_return *= (1.0 + ret)
                cumulative_return -= 1.0
            else:
                cumulative_return = 0.0

            future_returns.append({
                'symbol': stock_code,
                'date': pd.to_datetime(date_str, format='%Y%m%d'),
                'factor_minute': factor_minute,
                'return': cumulative_return,
                'minutes_used': len(stock_minute_returns),  # è®°å½•ä½¿ç”¨äº†å¤šå°‘åˆ†é’Ÿçš„æ•°æ®
                'requested_minutes': requested_minutes if 'requested_minutes' in locals() else 0
            })

        return pd.DataFrame(future_returns)

    except Exception as e:
        print(f"Error aggregating returns for minute {factor_minute}: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return pd.DataFrame()


def load_target_from_csv(start_date: str, end_date: str) -> Optional[Dict]:
    """
    ä»CSVæ–‡ä»¶åŠ è½½é¢„æµ‹ç›®æ ‡æ•°æ®
    æ–‡ä»¶æ ¼å¼: /dfs/dataset/10-1732512661487/data/Stocklabel_adj_lnret/YYYY/YYYYMMDD/eHHMMSS.csv

    åªå¤„ç†å®é™…å­˜åœ¨æ•°æ®çš„æ—¥æœŸï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„CSVæ–‡ä»¶ç›®å½•å†³å®šï¼‰

    Args:
        start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
        end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)

    Returns:
        åŒ…å«values, dates, symbolsçš„å­—å…¸ï¼Œä»¥åŠvalid_datesåˆ—è¡¨
    """
    try:
        # æ£€æŸ¥ç¼“å­˜
        cached_data = return_cache.load_returns(INTERVAL_CONFIG, start_date, end_date)
        if cached_data:
            print(f"Loaded cached target data for {INTERVAL_CONFIG}", file=sys.stderr)
            return cached_data

        from datetime import datetime, timedelta

        # è§£æintervalé…ç½®
        factor_interval, prediction_period = parse_interval_config(INTERVAL_CONFIG)

        # ç¬¬ä¸€æ­¥ï¼šæ‰«ææ•°æ®ç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰æ•ˆæ—¥æœŸ
        print(f"Scanning data directory for valid dates between {start_date} and {end_date}...", file=sys.stderr)

        valid_dates = []
        start = datetime.strptime(start_date, '%Y%m%d')
        end = datetime.strptime(end_date, '%Y%m%d')
        current = start

        while current <= end:
            date_str = current.strftime('%Y%m%d')
            year = date_str[:4]
            date_dir = os.path.join(FACTOR_DATA_ROOT_DIR, year, date_str)

            # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®ç›®å½•
            if os.path.exists(date_dir):
                # æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰CSVæ–‡ä»¶
                try:
                    csv_files = [f for f in os.listdir(date_dir) if f.startswith('e') and f.endswith('.csv')]
                    if csv_files:  # æœ‰CSVæ–‡ä»¶æ‰ç®—æœ‰æ•ˆæ—¥æœŸ
                        valid_dates.append(date_str)
                        print(f"  Found valid date: {date_str} ({len(csv_files)} files)", file=sys.stderr)
                except Exception as e:
                    print(f"  Warning: Failed to scan {date_dir}: {e}", file=sys.stderr)

            current += timedelta(days=1)

        if not valid_dates:
            print(f"No valid dates found with CSV data between {start_date} and {end_date}", file=sys.stderr)
            return None

        print(f"Found {len(valid_dates)} valid dates: {valid_dates[:5]}{'...' if len(valid_dates) > 5 else ''}", file=sys.stderr)

        # ç¬¬äºŒæ­¥ï¼šåªå¤„ç†æœ‰æ•ˆæ—¥æœŸçš„æ•°æ® (ä¼˜åŒ–ç‰ˆæœ¬)
        all_target_data = []

        for date_str in valid_dates:
            year = date_str[:4]
            date_dir = os.path.join(FACTOR_DATA_ROOT_DIR, year, date_str)

            try:
                print(f"Processing {date_str}: loading CSV files...", file=sys.stderr)
                start_time = time.time()

                # è·å–è¯¥æ—¥æœŸç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = [f for f in os.listdir(date_dir) if f.startswith('e') and f.endswith('.csv')]
                print(f"  Found {len(csv_files)} CSV files", file=sys.stderr)

                # é¢„è§£ææ‰€æœ‰æ–‡ä»¶çš„æ—¶é—´æˆ³
                file_info_list = []
                for csv_file in csv_files:
                    time_str = csv_file[1:-4]  # å»æ‰'e'å’Œ'.csv'
                    hour = int(time_str[:2])
                    minute = int(time_str[2:4])
                    second = int(time_str[4:])

                    # è®¡ç®—ä»9:30å¼€å§‹çš„åˆ†é’Ÿåç§»é‡
                    base_hour, base_minute = 9, 30
                    total_minutes = (hour - base_hour) * 60 + (minute - base_minute)
                    if total_minutes < 0:  # å¤„ç†è·¨å¤©æƒ…å†µ
                        total_minutes += 24 * 60

                    file_info_list.append({
                        'path': os.path.join(date_dir, csv_file),
                        'factor_minute': total_minutes
                    })

                # æ‰¹é‡è¯»å–å’Œå¤„ç†æ‰€æœ‰CSVæ–‡ä»¶
                batch_data = []
                for file_info in file_info_list:
                    try:
                        csv_path = file_info['path']
                        factor_minute = file_info['factor_minute']

                        # ä½¿ç”¨æ›´é«˜æ•ˆçš„å‚æ•°è¯»å–CSV
                        df = pd.read_csv(csv_path, usecols=['skey', TARGET_COLUMN, 'isZT', 'isDT'] if 'isZT' in pd.read_csv(csv_path, nrows=0).columns else ['skey', TARGET_COLUMN])

                        # è¿‡æ»¤æ¶¨è·Œåœæ•°æ®ï¼ˆå¦‚æœåˆ—å­˜åœ¨ï¼‰
                        if 'isZT' in df.columns and 'isDT' in df.columns:
                            df = df[(df['isZT'] == 0) & (df['isDT'] == 0)]

                        # å‘é‡åŒ–æ“ä½œï¼šç›´æ¥æ„å»ºæ•°æ®
                        date_timestamp = pd.to_datetime(date_str, format='%Y%m%d')

                        # ä½¿ç”¨å‘é‡åŒ–æ–¹å¼æ„å»ºæ•°æ®æ¡†
                        temp_df = pd.DataFrame({
                            'symbol': df['skey'],
                            'date': date_timestamp,
                            'factor_minute': factor_minute,
                            'return': df[TARGET_COLUMN]
                        })

                        batch_data.append(temp_df)

                    except Exception as e:
                        print(f"Warning: Failed to process {csv_path}: {e}", file=sys.stderr)
                        continue

                # æ‰¹é‡åˆå¹¶è¯¥æ—¥æœŸçš„æ‰€æœ‰æ•°æ®
                if batch_data:
                    try:
                        date_combined = pd.concat(batch_data, ignore_index=True)
                        # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—
                        required_columns = ['symbol', 'date', 'factor_minute', 'return']
                        if all(col in date_combined.columns for col in required_columns):
                            all_target_data.append(date_combined)
                        else:
                            print(f"  Warning: Missing required columns for {date_str}", file=sys.stderr)
                    except Exception as e:
                        print(f"  Warning: Failed to merge data for {date_str}: {e}", file=sys.stderr)

                    processing_time = time.time() - start_time
                    print(f"  Completed processing {date_str} in {processing_time:.2f}s, collected {len(date_combined)} records", file=sys.stderr)
                else:
                    print(f"  Warning: No data collected for {date_str}", file=sys.stderr)

            except Exception as e:
                print(f"Warning: Failed to process date {date_str}: {e}", file=sys.stderr)
                continue

        if not all_target_data:
            print("No target data collected from valid dates", file=sys.stderr)
            return None

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_target_data:
            combined_df = pd.concat(all_target_data, ignore_index=True)
            print(f"Total collected {len(combined_df)} records from {len(valid_dates)} dates", file=sys.stderr)
        else:
            print("No target data collected from valid dates", file=sys.stderr)
            return None

        # å¤„ç†æ•°æ®èšåˆ
        if factor_interval == "1d":
            # æ—¥çº§åˆ«ï¼šæŒ‰æ—¥æœŸèšåˆï¼Œåˆå¹¶æ‰€æœ‰æ—¶é—´ç‚¹çš„æ•°æ®
            pivot_df = combined_df.pivot_table(
                index='date',
                columns='symbol',
                values='return',
                aggfunc='mean'  # å¯¹åŒä¸€å¤©å¤šä¸ªæ—¶é—´ç‚¹å–å¹³å‡
            )
        else:
            # åˆ†é’Ÿçº§åˆ«ï¼šæŒ‰æ—¥æœŸ+å› å­åˆ†é’Ÿèšåˆ
            pivot_df = combined_df.pivot_table(
                index=['date', 'factor_minute'],
                columns='symbol',
                values='return',
                aggfunc='first'  # æ¯ä¸ªæ—¶é—´ç‚¹åº”è¯¥åªæœ‰ä¸€ä¸ªå€¼
            )

        # å¡«å……ç¼ºå¤±å€¼
        pivot_df = pivot_df.fillna(0.0)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        values = pivot_df.values
        dates = pivot_df.index
        symbols = pivot_df.columns

        # ä¿å­˜åˆ°ç¼“å­˜
        return_cache.save_returns(INTERVAL_CONFIG, start_date, end_date, values, dates, symbols)

        print(f"Loaded target data: {values.shape} using {INTERVAL_CONFIG} from column '{TARGET_COLUMN}'", file=sys.stderr)
        print(f"Valid dates processed: {len(valid_dates)} out of requested range", file=sys.stderr)

        return {
            'values': values,
            'dates': dates,
            'symbols': symbols,
            'valid_dates': valid_dates  # è¿”å›æœ‰æ•ˆæ—¥æœŸåˆ—è¡¨
        }

    except Exception as e:
        print(f"Error loading target from CSV: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return None


# ä¸ºäº†å‘åå…¼å®¹ï¼Œé‡å‘½åå‡½æ•°
load_target_from_parquet = load_target_from_csv





def calculate_ic_from_values(alpha_values: np.ndarray, target_values: np.ndarray) -> float:
    """
    ä»å› å­å€¼æ•°ç»„è®¡ç®—IC

    Args:
        alpha_values: alphaå› å­å€¼æ•°ç»„ (n_days, n_stocks)
        target_values: ç›®æ ‡å€¼æ•°ç»„ (n_days, n_stocks)

    Returns:
        ICå€¼ (float): çš®å°”é€Šç›¸å…³ç³»æ•°
    """
    try:
        # å±•å¹³æ•°ç»„
        alpha_flat = alpha_values.flatten()
        target_flat = target_values.flatten()

        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(alpha_flat) | np.isnan(target_flat))
        alpha_clean = alpha_flat[mask]
        target_clean = target_flat[mask]

        if len(alpha_clean) == 0:
            return 0.0

        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation_matrix = np.corrcoef(alpha_clean, target_clean)
        ic = correlation_matrix[0, 1]

        # æ£€æŸ¥æ˜¯å¦ä¸ºNaN
        return 0.0 if np.isnan(ic) else ic

    except Exception as e:
        print(f"Error calculating IC from values: {e}", file=sys.stderr)
        return 0.0


def calculate_alpha_ic(alpha_expr: str, target_expr: str = None) -> float:
    """
    è®¡ç®—alphaè¡¨è¾¾å¼ä¸ç›®æ ‡çš„IC
    ä»å·²ç¼“å­˜çš„å› å­æ•°æ®ä¸­è¯»å–è®¡ç®—

    Args:
        alpha_expr: Alphaè¡¨è¾¾å¼å­—ç¬¦ä¸²
        target_expr: ç›®æ ‡è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼Œç”¨äºæŒ‡å®šç‰¹å®šç›®æ ‡ï¼‰

    Returns:
        ICå€¼ (float): ä¿¡æ¯ç³»æ•°
    """
    try:
        print(f"ğŸ” calculate_alpha_ic called for: {alpha_expr}", file=sys.stderr)

        # ç›´æ¥ä»ç¼“å­˜åŠ è½½alphaå› å­å€¼ï¼ˆå·²ç”±external_compute_factoré¢„å…ˆè®¡ç®—å¹¶ç¼“å­˜ï¼‰
        alpha_data = factor_cache.load_factor(alpha_expr)
        if alpha_data is None:
            print(f"âŒ Alpha factor not found in cache: {alpha_expr}", file=sys.stderr)
            print("ğŸ’¡ Make sure external_compute_factor has been called for this expression first", file=sys.stderr)
            return 0.0

        print(f"âœ… Found cached alpha data: shape {alpha_data['values'].shape}", file=sys.stderr)

        alpha_values = alpha_data['values']

        # åŠ è½½ç›®æ ‡å€¼
        if target_expr:
            # ä½¿ç”¨æŒ‡å®šçš„ç›®æ ‡è¡¨è¾¾å¼
            target_data = factor_cache.load_factor(target_expr)
            if target_data is None:
                print(f"Target factor not found in cache: {target_expr}", file=sys.stderr)
                return 0.0
            target_values = target_data['values']
        else:
            # ä½¿ç”¨é»˜è®¤ç›®æ ‡ï¼ˆæœªæ¥æ”¶ç›Šç‡ï¼‰
            target_data = target_manager.load_target()
            if target_data is None:
                # å°è¯•ä»parquetæ–‡ä»¶åŠ è½½
                print("Loading target from parquet files...", file=sys.stderr)
                # ä»alphaæ•°æ®è·å–æ—¥æœŸèŒƒå›´
                alpha_dates = alpha_data['dates']
                if len(alpha_dates) > 0:
                    start_date = alpha_dates.min().strftime('%Y%m%d')
                    end_date = alpha_dates.max().strftime('%Y%m%d')
                    target_data = load_target_from_parquet(start_date, end_date)

                if target_data is None:
                    print("Default target values not found. Make sure target has been pre-computed or parquet files are available.", file=sys.stderr)
                    return 0.0

            target_values = target_data['values']

        # ç¡®ä¿ç»´åº¦åŒ¹é…
        if alpha_values.shape != target_values.shape:
            print(f"Shape mismatch: alpha {alpha_values.shape} vs target {target_values.shape}", file=sys.stderr)
            return 0.0

        # è®¡ç®—IC
        ic = calculate_ic_from_values(alpha_values, target_values)
        return ic

    except Exception as e:
        print(f"Error calculating IC for expression {alpha_expr}: {e}", file=sys.stderr)
        return 0.0


def evaluate_factor_combination_lgb(exprs: list, baseline_exprs: list = None) -> dict:
    """
    ä½¿ç”¨LightGBMè¯„ä¼°å› å­ç»„åˆçš„é¢„æµ‹èƒ½åŠ› (ç±»ä¼¼lgb_baselineçš„æ–¹å¼)
    è¿”å›è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸»è¦å…³æ³¨q5è¡¨ç°

    Args:
        exprs: æ–°å¢çš„è¡¨è¾¾å¼åˆ—è¡¨
        baseline_exprs: åŸºå‡†è¡¨è¾¾å¼åˆ—è¡¨ï¼ˆå·²æœ‰å› å­ï¼‰

    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«IC, RankIC, q5æ”¶ç›Š, q1æ”¶ç›Šç­‰
    """
    try:
        from sklearn.model_selection import train_test_split
        import lightgbm as lgb
        from scipy.stats import spearmanr

        # åŠ è½½æ‰€æœ‰å› å­æ•°æ®
        all_exprs = (baseline_exprs or []) + exprs
        if not all_exprs:
            return {"error": "No expressions provided"}

        factor_data_list = []
        for expr in all_exprs:
            data = factor_cache.load_factor(expr)
            if data is None:
                return {"error": f"Factor not found: {expr}"}
            factor_data_list.append(data)

        # ç¡®ä¿æ‰€æœ‰å› å­æœ‰ç›¸åŒçš„ç»´åº¦
        base_shape = factor_data_list[0]['values'].shape
        if not all(data['values'].shape == base_shape for data in factor_data_list):
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•æŒ‰æ—¥æœŸå¯¹é½
            print(f"âš ï¸ Factor dimensions don't match, attempting to align by date...")
            try:
                aligned_data = []
                base_dates = factor_data_list[0]['dates']
                base_symbols = factor_data_list[0]['symbols']

                for data in factor_data_list:
                    # å¯¹é½æ—¥æœŸå’Œè‚¡ç¥¨
                    common_dates = base_dates.intersection(data['dates'])
                    common_symbols = base_symbols.intersection(data['symbols'])

                    if len(common_dates) == 0 or len(common_symbols) == 0:
                        return {"error": f"Cannot align factor data: no common dates/symbols"}

                    # é‡æ–°ç´¢å¼•æ•°æ®
                    df = pd.DataFrame(data['values'], index=data['dates'], columns=data['symbols'])
                    aligned_df = df.loc[common_dates, common_symbols]

                    aligned_data.append({
                        'values': aligned_df.values,
                        'dates': aligned_df.index,
                        'symbols': aligned_df.columns
                    })

                factor_data_list = aligned_data
                base_shape = factor_data_list[0]['values'].shape
                print(f"âœ… Successfully aligned factor data to shape: {base_shape}")

            except Exception as e:
                return {"error": f"Failed to align factor dimensions: {str(e)}"}

        # åˆå¹¶å› å­æ•°æ®ä½œä¸ºç‰¹å¾
        X = np.stack([data['values'] for data in factor_data_list], axis=-1)  # (n_days, n_stocks, n_factors)
        n_days, n_stocks, n_factors = X.shape

        # åŠ è½½ç›®æ ‡å€¼å¹¶ç¡®ä¿ç»´åº¦åŒ¹é…
        target_data = target_manager.load_target()
        if target_data is None:
            return {"error": "Target values not found"}

        # æ£€æŸ¥ç›®æ ‡æ•°æ®ç»´åº¦æ˜¯å¦ä¸å› å­æ•°æ®åŒ¹é…
        target_values = target_data['values']
        target_dates = target_data['dates']
        target_symbols = target_data['symbols']

        factor_dates = factor_data_list[0]['dates']
        factor_symbols = factor_data_list[0]['symbols']

        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•å¯¹é½
        if target_values.shape != (n_days, n_stocks):
            print(f"âš ï¸ Target shape {target_values.shape} doesn't match factors {(n_days, n_stocks)}, attempting alignment...")

            # åˆ›å»ºç›®æ ‡æ•°æ®DataFrame
            target_df = pd.DataFrame(target_values, index=target_dates, columns=target_symbols)

            # å¯¹é½åˆ°å› å­æ•°æ®çš„æ—¥æœŸå’Œè‚¡ç¥¨
            common_dates = factor_dates.intersection(target_dates)
            common_symbols = factor_symbols.intersection(target_symbols)

            if len(common_dates) == 0 or len(common_symbols) == 0:
                return {"error": "Cannot align target data with factor data: no common dates/symbols"}

            aligned_target_df = target_df.loc[common_dates, common_symbols]
            y = aligned_target_df.values

            # åŒæ—¶è°ƒæ•´å› å­æ•°æ®çš„ç»´åº¦
            factor_df = pd.DataFrame(X[:, :, 0], index=factor_dates, columns=factor_symbols)
            aligned_factor_df = factor_df.loc[common_dates, common_symbols]

            n_days, n_stocks = aligned_factor_df.shape
            X = np.stack([aligned_factor_df.values] + [data['values'] for data in factor_data_list[1:]], axis=-1)

            print(f"âœ… Successfully aligned data to shape: factors {(n_days, n_stocks, n_factors)}, target {(n_days, n_stocks)}")
        else:
            y = target_values  # (n_days, n_stocks)

        # å±•å¹³æ•°æ®ç”¨äºLightGBM
        X_flat = X.reshape(n_days * n_stocks, n_factors)
        y_flat = y.reshape(n_days * n_stocks)

        # ç§»é™¤NaNå€¼
        valid_mask = ~(np.isnan(X_flat).any(axis=1) | np.isnan(y_flat))
        X_flat = X_flat[valid_mask]
        y_flat = y_flat[valid_mask]

        if len(X_flat) < 1000:  # æ ·æœ¬å¤ªå°‘
            return {"error": "Insufficient data for evaluation"}

        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_flat, y_flat, test_size=0.2, random_state=42
        )

        # è®­ç»ƒLightGBMæ¨¡å‹
        train_data = lgb.Dataset(X_train, label=y_train)
        params = {
            'objective': 'regression',
            'metric': 'mse',
            'boosting_type': 'gbdt',
            'num_leaves': min(31, max(10, n_factors * 2)),
            'learning_rate': 0.05,
            'feature_fraction': min(0.9, max(0.5, n_factors / 20.0)),
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'num_threads': 4
        }

        model = lgb.train(params, train_data, num_boost_round=100)

        # é¢„æµ‹
        y_pred = model.predict(X_test)

        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        ic = calculate_ic_from_values(y_pred.reshape(-1, 1), y_test.reshape(-1, 1))
        rank_ic = spearmanr(y_pred, y_test)[0] if len(y_pred) > 10 else 0.0

        # è®¡ç®—q5-q1æ”¶ç›Šå·®å¼‚ (æ ¸å¿ƒæŒ‡æ ‡)
        n_samples = len(y_pred)
        if n_samples >= 20:  # è‡³å°‘éœ€è¦20ä¸ªæ ·æœ¬
            # æŒ‰é¢„æµ‹å€¼æ’åº
            sorted_indices = np.argsort(y_pred)

            # q5: æœ€å¥½çš„20%
            q5_indices = sorted_indices[int(0.8 * n_samples):]
            q5_return = np.mean(y_test[q5_indices])

            # q1: æœ€å·®çš„20%
            q1_indices = sorted_indices[:int(0.2 * n_samples)]
            q1_return = np.mean(y_test[q1_indices])

            # q5-q1å·®å¼‚ (bps)
            q5_q1_diff = (q5_return - q1_return) * 10000  # è½¬æ¢ä¸ºbps

            # åˆ†ä½æ•°æ”¶ç›Š
            q5_return_bps = q5_return * 10000
            q1_return_bps = q1_return * 10000

        else:
            q5_return_bps = q1_return_bps = q5_q1_diff = 0.0

        # è®¡ç®—å› å­é‡è¦æ€§ (å¦‚æœæœ‰æ–°å¢å› å­)
        feature_importance = {}
        if exprs and baseline_exprs:
            # è®¡ç®—æ–°å¢å› å­çš„è¾¹é™…è´¡çŒ®
            try:
                importance = model.feature_importance(importance_type='gain')
                baseline_count = len(baseline_exprs)

                # æ–°å¢å› å­çš„å¹³å‡é‡è¦æ€§
                if len(exprs) > 0:
                    new_factor_importance = np.mean(importance[baseline_count:])
                    baseline_importance = np.mean(importance[:baseline_count]) if baseline_count > 0 else 0
                    importance_ratio = new_factor_importance / max(baseline_importance, 1e-6)
                    feature_importance = {
                        "new_factor_avg_importance": new_factor_importance,
                        "baseline_avg_importance": baseline_importance,
                        "importance_ratio": importance_ratio
                    }
            except:
                pass

        return {
            "ic": ic if not np.isnan(ic) else 0.0,
            "rank_ic": rank_ic if not np.isnan(rank_ic) else 0.0,
            "q5_return_bps": q5_return_bps,
            "q1_return_bps": q1_return_bps,
            "q5_q1_diff_bps": q5_q1_diff,  # æ ¸å¿ƒæŒ‡æ ‡
            "n_factors": n_factors,
            "n_samples": len(X_flat),
            "feature_importance": feature_importance
        }

    except Exception as e:
        return {"error": f"Error in LGB evaluation: {str(e)}"}


def calculate_factor_reward(metrics: dict, prev_metrics: dict = None) -> float:
    """
    æ ¹æ®è¯„ä¼°æŒ‡æ ‡è®¡ç®—RLå¥–åŠ±

    Args:
        metrics: å½“å‰å› å­ç»„åˆçš„è¯„ä¼°æŒ‡æ ‡
        prev_metrics: ä¹‹å‰çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆç”¨äºè®¡ç®—å¢é‡ï¼‰

    Returns:
        å¥–åŠ±å€¼
    """
    if "error" in metrics:
        return -1.0  # é”™è¯¯æƒ…å†µç»™äºˆæƒ©ç½š

    # ä¸»è¦å…³æ³¨q5æ”¶ç›Šè¡¨ç°
    q5_reward = metrics.get("q5_return_bps", 0.0) * 0.01  # q5æ”¶ç›Šæƒé‡

    # ICçš„è´¡çŒ® (è¾…åŠ©æŒ‡æ ‡)
    ic_reward = metrics.get("ic", 0.0) * 0.5

    # q5-q1å·®å¼‚çš„è´¡çŒ® (é€‰è‚¡èƒ½åŠ›)
    q5_q1_reward = max(0, metrics.get("q5_q1_diff_bps", 0.0)) * 0.005

    # å› å­é‡è¦æ€§å¥–åŠ± (æ–°å¢å› å­çš„è´¡çŒ®)
    importance_reward = 0.0
    if "feature_importance" in metrics:
        importance_ratio = metrics["feature_importance"].get("importance_ratio", 1.0)
        if importance_ratio > 1.2:  # æ–°å› å­æ˜æ˜¾æ›´æœ‰è´¡çŒ®
            importance_reward = (importance_ratio - 1.0) * 0.1

    # åŸºç¡€å¥–åŠ±
    base_reward = q5_reward + ic_reward + q5_q1_reward + importance_reward

    # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œè®¡ç®—å¢é‡å¥–åŠ±
    if prev_metrics and "error" not in prev_metrics:
        prev_q5 = prev_metrics.get("q5_return_bps", 0.0)
        current_q5 = metrics.get("q5_return_bps", 0.0)
        q5_increment = current_q5 - prev_q5

        # å¢é‡å¥–åŠ± (æ›´å¤§çš„æƒé‡)
        increment_reward = q5_increment * 0.02

        return base_reward + increment_reward

    return base_reward


def calculate_pool_ic_lgb(exprs: list, baseline_exprs: list = None) -> float:
    """
    å…¼å®¹æ€§å‡½æ•°ï¼šè¿”å›å¥–åŠ±å€¼ç”¨äºAlphaPool
    """
    metrics = evaluate_factor_combination_lgb(exprs, baseline_exprs)
    return calculate_factor_reward(metrics)


def calculate_pool_ic(exprs: list, weights: list) -> float:
    """
    è®¡ç®—å› å­æ± çš„ç»„åˆIC (ä¼ ç»Ÿæ–¹æ³•)

    Args:
        exprs: è¡¨è¾¾å¼åˆ—è¡¨
        weights: æƒé‡åˆ—è¡¨

    Returns:
        ç»„åˆICå€¼
    """
    try:
        if not exprs or not weights:
            return 0.0

        # åŠ è½½æ‰€æœ‰å› å­å€¼
        factor_values = []
        for expr in exprs:
            data = factor_cache.load_factor(expr)
            if data is None:
                print(f"Factor not found: {expr}", file=sys.stderr)
                return 0.0
            factor_values.append(data['values'])

        # è®¡ç®—åŠ æƒç»„åˆ
        combined_values = np.zeros_like(factor_values[0])
        for values, weight in zip(factor_values, weights):
            combined_values += values * weight

        # åŠ è½½ç›®æ ‡å€¼
        target_data = target_manager.load_target()
        if target_data is None:
            print("Target values not found", file=sys.stderr)
            return 0.0

        target_values = target_data['values']

        # è®¡ç®—ç»„åˆIC
        ic = calculate_ic_from_values(combined_values, target_values)
        return ic

    except Exception as e:
        print(f"Error calculating pool IC: {e}", file=sys.stderr)
        return 0.0


def main():
    """
    ä¸»å‡½æ•°ï¼šä»å‘½ä»¤è¡Œå‚æ•°æ¥æ”¶è¡¨è¾¾å¼å¹¶è®¡ç®—IC
    ç”¨æ³•ï¼š
    python scoring_calculator.py <alpha_expression> [target_expression]
    python scoring_calculator.py --pool <expr1> <weight1> <expr2> <weight2> ...
    """
    if len(sys.argv) < 2:
        print("Usage:", file=sys.stderr)
        print("  Single IC: python scoring_calculator.py <alpha_expression> [target_expression]", file=sys.stderr)
        print("  Pool IC: python scoring_calculator.py --pool <expr1> <weight1> <expr2> <weight2> ...", file=sys.stderr)
        sys.exit(1)

    if sys.argv[1] == '--pool':
        # è®¡ç®—pool IC
        if len(sys.argv) < 4 or (len(sys.argv) - 2) % 2 != 0:
            print("Pool mode requires pairs of expression and weight", file=sys.stderr)
            sys.exit(1)

        exprs = []
        weights = []
        for i in range(2, len(sys.argv), 2):
            exprs.append(sys.argv[i])
            weights.append(float(sys.argv[i + 1]))

        ic = calculate_pool_ic(exprs, weights)
    else:
        # è®¡ç®—å•ä¸ªIC
        alpha_expr = sys.argv[1]
        target_expr = sys.argv[2] if len(sys.argv) > 2 else None
        ic = calculate_alpha_ic(alpha_expr, target_expr)

    print(f"{ic:.6f}")


if __name__ == "__main__":
    main()
