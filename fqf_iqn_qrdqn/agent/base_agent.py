from abc import ABC, abstractmethod
import os
import time
import pandas as pd
import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter

from fqf_iqn_qrdqn.memory import LazyMultiStepMemory, \
    LazyPrioritizedMultiStepMemory
from fqf_iqn_qrdqn.utils import RunningMeanStats, LinearAnneaer


class BaseAgent(ABC):

    def __init__(self, env, valid_calculator, test_calculator, log_dir, num_steps=5*(10**7),
                 batch_size=32, memory_size=10**6, gamma=0.99, multi_step=1,
                 update_interval=4, target_update_interval=10000,
                 start_steps=50000, epsilon_train=0.01, epsilon_eval=0.001,
                 epsilon_decay_steps=250000, double_q_learning=False,
                 dueling_net=False, noisy_net=False, use_per=False,
                 log_interval=100, eval_interval=250000, num_eval_steps=125000,
                 max_episode_steps=27000, grad_cliping=5.0, cuda=True, seed=0,
                 num_parallel_envs=1):  # æ–°å¢ï¼šå¹¶è¡Œç¯å¢ƒæ•°é‡

        # æš‚å­˜å¾…è¡¥å…¨rewardçš„transitions
        self.pending_transitions = []  # æ ¼å¼: dict with state, action, next_state, done, reward_type, episode_ref

        self.env = env
        self.num_parallel_envs = num_parallel_envs
        self.valid_calculator = valid_calculator
        self.test_calculator = test_calculator

        torch.manual_seed(seed)
        np.random.seed(seed)
        # torch.backends.cudnn.deterministic = True  # It harms a performance.
        # torch.backends.cudnn.benchmark = False  # It harms a performance.

        self.device = torch.device(
            "cuda" if cuda and torch.cuda.is_available() else "cpu")

        self.online_net = None
        self.target_net = None
        self.online_mean_net = None
        self.target_mean_net = None

        # Replay memory which is memory-efficient to store stacked frames.
        if use_per:
            beta_steps = (num_steps - start_steps) / update_interval
            self.memory = LazyPrioritizedMultiStepMemory(
                memory_size, self.env.observation_space.shape,
                self.device, gamma, multi_step, beta_steps=beta_steps)
        else:
            self.memory = LazyMultiStepMemory(
                memory_size, self.env.observation_space.shape,
                self.device, gamma, multi_step)

        self.log_dir = log_dir
        self.model_dir = os.path.join(log_dir, 'model')
        self.summary_dir = os.path.join(log_dir, 'summary')
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)
        if not os.path.exists(self.summary_dir):
            os.makedirs(self.summary_dir)

        self.writer = SummaryWriter(log_dir=self.summary_dir)
        self.train_return = RunningMeanStats(log_interval)

        self.steps = 0
        self.learning_steps = 0
        self.episodes = 0
        self.best_eval_score = -np.inf
        self.best_test_score = -np.inf
        self.num_actions = self.env.action_space.n
        self.num_steps = num_steps
        self.batch_size = batch_size

        self.double_q_learning = double_q_learning
        self.dueling_net = dueling_net
        self.noisy_net = noisy_net
        self.use_per = use_per

        self.log_interval = log_interval
        self.eval_interval = eval_interval
        self.num_eval_steps = num_eval_steps
        self.gamma_n = gamma ** multi_step
        self.start_steps = start_steps
        self.epsilon_train = LinearAnneaer(1.0, epsilon_train, epsilon_decay_steps)
        self.epsilon_eval = epsilon_eval
        self.update_interval = update_interval
        self.target_update_interval = target_update_interval
        self.max_episode_steps = max_episode_steps
        self.grad_cliping = grad_cliping

    def run(self):
        while True:
            self.train_episode()
            if self.steps > self.num_steps:
                break

    def is_update(self):
        return self.steps % self.update_interval == 0\
            and self.steps >= self.start_steps\
            and len(self.memory) >= self.batch_size

    def is_random(self, eval=False):
        # Use e-greedy for evaluation.
        if self.steps < self.start_steps:
            return True
        if eval:
            return np.random.rand() < self.epsilon_eval
        if self.noisy_net:
            return False

        # è¯­æ³•å­¦ä¹ é˜¶æ®µå¢å¼ºéšæœºæ€§ï¼Œé¼“åŠ±æ¢ç´¢å¤šå­—æ®µè¡¨è¾¾å¼
        import os
        is_syntax_learning = os.environ.get('ALPHAQCM_SYNTAX_LEARNING', '').lower() == 'true'
        if is_syntax_learning:
            # è¯­æ³•å­¦ä¹ é˜¶æ®µï¼šå¤§å¹…æé«˜éšæœºè¡ŒåŠ¨æ¦‚ç‡è‡³50%
            return np.random.rand() < 0.5  # å›ºå®š50%éšæœºæ€§
        else:
            # ICå­¦ä¹ é˜¶æ®µï¼šæ­£å¸¸epsilon
            return np.random.rand() < self.epsilon_train.get()

    def update_target(self):
        self.target_net.load_state_dict(
            self.online_net.state_dict())

    def explore(self):
        # Act with randomness.
        allowed_action = self.env.action_masks()

        # ç§»é™¤å¼ºåˆ¶å­—æ®µé€‰æ‹©çš„é™åˆ¶ï¼Œè®©RLè‡ªç”±æ¢ç´¢

        # ç‰¹æ®Šå¤„ç†SEPåŠ¨ä½œï¼šå½“SEPå¯ç”¨æ—¶ï¼Œç»™å®ƒæ›´é«˜çš„é€‰æ‹©æ¦‚ç‡
        if hasattr(self.env, 'sep_action') and self.env.sep_action is not None:
            sep_idx = self.env.sep_action
            if allowed_action[sep_idx]:
                # SEPå¯ç”¨æ—¶ï¼Œæœ‰30%çš„æ¦‚ç‡ç›´æ¥é€‰æ‹©SEP
                if np.random.rand() < 0.3:
                    return sep_idx

        # æ­£å¸¸éšæœºé€‰æ‹©ï¼ˆå¯èƒ½ä½¿ç”¨å—é™çš„åŠ¨ä½œç©ºé—´ï¼‰
        action = self.env.action_space.sample()
        while not allowed_action[action]:
            action = self.env.action_space.sample()
        return action

    def exploit(self, state):
        # Act without randomness.
        state = torch.ByteTensor(state).unsqueeze(0).to(self.device).float()
        with torch.no_grad():
            q_values = self.online_net.calculate_q(states=state)
            forbid_action = torch.BoolTensor(
                ~self.env.action_masks()).to(self.device)
            q_values[:, forbid_action] = -1e6
            action = q_values.argmax().item()
        return action

    @abstractmethod
    def learn(self):
        """Learn from experience. Must be implemented by subclasses."""
        raise NotImplementedError("learn() method must be implemented by subclasses")

    def save_models(self, save_dir):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        torch.save(
            self.online_net.state_dict(),
            os.path.join(save_dir, 'online_net.pth'))
        torch.save(
            self.target_net.state_dict(),
            os.path.join(save_dir, 'target_net.pth'))
        if (self.online_mean_net is not None) & (self.target_mean_net is not None):
            torch.save(
                self.online_mean_net.state_dict(),
                os.path.join(save_dir, 'online_mean_net.pth'))
            torch.save(
                self.target_mean_net.state_dict(),
                os.path.join(save_dir, 'target_mean_net.pth'))
        

    def load_models(self, save_dir, require_mean = False):
        self.online_net.load_state_dict(torch.load(
            os.path.join(save_dir, 'online_net.pth')))
        self.target_net.load_state_dict(torch.load(
            os.path.join(save_dir, 'target_net.pth')))
        
        if require_mean:
            self.online_net.load_state_dict(torch.load(
                os.path.join(save_dir, 'online_mean_net.pth')))
            self.target_net.load_state_dict(torch.load(
                os.path.join(save_dir, 'target_mean_net.pth')))

    def save_exprs(self, save_dir, valid_ic, test_ic, set_indice):
        state = self.env.pool.state
        n = len(state['exprs'])

        log_table = pd.DataFrame(
            columns=['exprs', 'ic', 'weight'], index=range(n + 1))

        for i in range(n):
            weight = state['weights'][i]
            expr_str = str(state['exprs'][i])
            ic_ret = state['ics_ret'][i]

            log_table.loc[i, :] = (expr_str, ic_ret, weight)

        if set_indice == 'test':
            
            log_table.loc[n, :] = ('Ensemble', valid_ic, test_ic)
            log_table.to_csv(f'{save_dir}/test_best_table.csv')
        elif set_indice == 'valid':
            log_table.loc[n, :] = ('Ensemble', valid_ic, test_ic)
            log_table.to_csv(f'{save_dir}/valid_best_table.csv')

    # def save_agent(self, save_dir):
    #     if not os.path.exists(save_dir):
    #         os.makedirs(save_dir)
    #     torch.save({'train_return': self.train_return, 'steps': self.steps,
    #                 'learning_steps': self.learning_steps, 'episodes': self.episodes,
    #                 'best_eval_score': self.best_eval_score, 'epsilon_train': self.epsilon_train,
    #                 'optim_online': self.optim_online.state_dict()},
    #                os.path.join(save_dir, 'agent.pkl'))

    # def load_agent(self, save_dir):
    #     checkpoint = torch.load(os.path.join(save_dir, 'agent.pkl'))
    #     self.train_return = checkpoint['train_return']
    #     self.steps = checkpoint['steps']
    #     self.learning_steps = checkpoint['learning_steps']
    #     self.episodes = checkpoint['episodes']
    #     self.best_eval_score = checkpoint['best_eval_score']
    #     self.epsilon_train = checkpoint['epsilon_train']
    #     self.optim_online.load_state_dict(checkpoint['optim_online'])

    def train_episode(self):
        self.online_net.train()
        self.target_net.train()

        self.episodes += 1
        episode_return = 0.
        episode_steps = 0

        done = False
        try:
            state, info = self.env.reset()
        except Exception as e:
            print(f"âŒ Failed to reset environment: {e}")
            import traceback
            traceback.print_exc()
            return  # Skip this episode

        while (not done) and episode_steps <= self.max_episode_steps:
            try:
                self.online_net.sample_noise()

                if self.is_random(eval=False):
                    action = self.explore()
                else:
                    action = self.exploit(state)

                next_state, reward, done, _, info = self.env.step(action)

                self.memory.append(state, action, reward, next_state, done)

                self.steps += 1
                episode_steps += 1
                episode_return += reward
                state = next_state

                self.train_step_interval()

            except Exception as e:
                print(f"âŒ Error during training step {episode_steps}: {e}")
                import traceback
                traceback.print_exc()
                print("Continuing with next episode...")
                episode_return -= 10.0
                break  # End this episode

        # Only print episode summary without step details

        # We log running mean of stats.
        self.train_return.append(episode_return)

        # We log evaluation results along with training steps.
        if self.episodes % self.log_interval == 0:
            try:
                self.writer.add_scalar(
                    'ic/train', self.env.env.pool.state['best_ic_ret'], self.steps)
            except Exception as e:
                print(f"Warning: Failed to log training stats: {e}")

    def train_step_interval(self):
        self.epsilon_train.step()

        if self.steps % self.target_update_interval == 0:
            self.update_target()

        # å¤„ç†æ‰€æœ‰å·²å®Œæˆçš„å¼‚æ­¥rewardè®¡ç®—
        self._process_completed_async_rewards()

        if self.is_update():
            self.learn()

    def _complete_pending_transitions(self, episode, final_reward, reward_type):
        """è¡¥å…¨æŒ‡å®šepisodeçš„pending transitions"""
        # æ‰¾åˆ°è¯¥episodeçš„æ‰€æœ‰pending transitions
        transitions_to_complete = [
            t for t in self.pending_transitions
            if t['episode_ref'] is episode and t['reward_type'] == reward_type
        ]

        # è¡¥å…¨å¹¶ä¿å­˜åˆ°Replay Memory
        for transition in transitions_to_complete:
            self.memory.append(
                transition['state'],
                transition['action'],
                final_reward,  # ä½¿ç”¨è®¡ç®—å‡ºçš„æœ€ç»ˆreward
                transition['next_state'],
                transition['done']
            )

            # ä»pendingåˆ—è¡¨ä¸­ç§»é™¤
            self.pending_transitions.remove(transition)

    def _process_completed_async_rewards(self):
        """å¤„ç†æ‰€æœ‰å·²å®Œæˆçš„å¼‚æ­¥rewardè®¡ç®—"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„å¼‚æ­¥rewardå¤„ç†é€»è¾‘
        # ç›®å‰ä¸»è¦é€šè¿‡_complete_pending_transitionså¤„ç†
        pass

    def evaluate(self):
        try:
            valid_ic = self.env.env.pool.test_ensemble(self.valid_calculator)
            test_ic = self.env.env.pool.test_ensemble(self.test_calculator)

            if valid_ic > self.best_eval_score:
                self.best_eval_score = valid_ic
                self.save_models(os.path.join(self.model_dir, 'best'))
                self.save_exprs(self.log_dir, valid_ic, test_ic, 'valid')

            if test_ic > self.best_test_score:
                self.best_test_score = test_ic
                self.save_exprs(self.log_dir, valid_ic, test_ic, 'test')

            # We log evaluation results along with training steps.
            self.writer.add_scalar('ic/valid', valid_ic, self.steps)
            self.writer.add_scalar('ic/test', test_ic, self.steps)

        except Exception as e:
            print(f"Warning: Evaluation failed: {e}")
            # Continue training even if evaluation fails

    def __del__(self):
        try:
            if hasattr(self, 'env') and self.env is not None:
                self.env.close()
        except:
            pass  # Ignore errors during cleanup

        try:
            if hasattr(self, 'writer') and self.writer is not None:
                self.writer.close()
        except:
            pass  # Ignore errors during cleanup


class BatchAgent(BaseAgent):
    """æ”¯æŒå¤šepisodeæ‰¹å¤„ç†çš„Agent"""

    def __init__(self, env_template, valid_calculator, test_calculator, log_dir,
                 num_parallel_envs=4, batch_final_threshold=None, **kwargs):
        # åˆ›å»ºå¤šä¸ªç¯å¢ƒçš„å‰¯æœ¬
        self.env_template = env_template
        self.envs = [self._create_env_copy() for _ in range(num_parallel_envs)]
        self.num_parallel_envs = num_parallel_envs

        # çŠ¶æ€2æ‰¹é‡è®¡ç®—ç›¸å…³
        # é»˜è®¤ä¸å¹¶è¡Œç¯å¢ƒæ•°é‡ä¸€è‡´ï¼Œæœ€å°åŒ–å»¶è¿Ÿ
        self.batch_final_threshold = batch_final_threshold or num_parallel_envs
        self.pending_final_expressions = []  # ç­‰å¾…æ‰¹é‡è®¡ç®—çš„çŠ¶æ€2è¡¨è¾¾å¼

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªç¯å¢ƒä½œä¸ºä»£è¡¨è¿›è¡Œåˆå§‹åŒ–
        super().__init__(self.envs[0], valid_calculator, test_calculator, log_dir,
                        num_parallel_envs=num_parallel_envs, **kwargs)

    def _create_env_copy(self):
        """åˆ›å»ºç¯å¢ƒçš„ç‹¬ç«‹å‰¯æœ¬"""
        # è¿™é‡Œéœ€è¦å®ç°ç¯å¢ƒçš„æ·±æ‹·è´æˆ–ç‹¬ç«‹å®ä¾‹åŒ–
        # ç›®å‰ç®€åŒ–å¤„ç†ï¼Œè¿”å›æ¨¡æ¿ç¯å¢ƒï¼ˆåœ¨å®é™…å®ç°ä¸­éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
        return self.env_template

    def run(self):
        """æ‰¹å¤„ç†è®­ç»ƒä¸»å¾ªç¯"""
        while True:
            self.train_batch_episodes()
            if self.steps > self.num_steps:
                break

    def train_batch_episodes(self):
        """å¹¶è¡Œè®­ç»ƒå¤šä¸ªepisodes"""
        # åˆå§‹åŒ–æ‰€æœ‰episodes
        episode_states = []
        for i, env in enumerate(self.envs):
            try:
                state, info = env.reset()
                episode_states.append({
                    'env_idx': i,
                    'env': env,
                    'state': state,
                    'done': False,
                    'return': 0.0,
                    'steps': 0,
                    'status': 'running'  # running, waiting_intermediate, waiting_final, terminated
                })
            except Exception as e:
                print(f"âŒ Failed to reset env {i}: {e}")
                continue

        active_episodes = episode_states.copy()

        while active_episodes:
            # é˜¶æ®µ1: å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ´»è·ƒepisodes
            completed_episodes = []
            waiting_intermediate = []
            waiting_final = []
            terminated_episodes = []

            for episode in active_episodes[:]:  # å¤åˆ¶åˆ—è¡¨ä»¥ä¾¿ä¿®æ”¹
                if episode['status'] == 'running':
                    try:
                        # é€‰æ‹©åŠ¨ä½œ
                        if self.is_random(eval=False):
                            action = self.explore_for_env(episode['env'])
                        else:
                            action = self.exploit_for_env(episode['env'], episode['state'])

                        # æ‰§è¡ŒåŠ¨ä½œ
                        next_state, reward, done, truncated, info = episode['env'].step(action)

                        # åˆ¤æ–­rewardæ˜¯å¦å¯ä»¥ç«‹å³ç¡®å®šå¹¶ä¿å­˜transition
                        needs_async_reward = (done and not info.get('terminated_by_invalid')) or info.get('waiting_for_ic')

                        if needs_async_reward:
                            # éœ€è¦å¼‚æ­¥è®¡ç®—rewardï¼šæš‚å­˜transitionï¼Œç­‰å¾…è¡¥å…¨
                            pending_transition = {
                                'episode_idx': episode['env_idx'],
                                'state': episode['state'],
                                'action': action,
                                'next_state': next_state,
                                'done': done,
                                'reward_type': 'waiting_final' if done else 'waiting_intermediate',
                                'episode_ref': episode  # å¼•ç”¨episodeä»¥ä¾¿åç»­æ›´æ–°
                            }
                            self.pending_transitions.append(pending_transition)
                        else:
                            # rewardå¯ä»¥ç«‹å³ç¡®å®šï¼šç›´æ¥ä¿å­˜åˆ°Replay Memory
                            self.memory.append(episode['state'], action, reward, next_state, done)

                        # æ›´æ–°çŠ¶æ€
                        self.steps += 1
                        episode['steps'] += 1
                        episode['return'] += reward
                        episode['state'] = next_state

                        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åŒæ­¥ç‚¹
                        if done:
                            if info.get('terminated_by_invalid'):
                                # çŠ¶æ€1: æ— æ•ˆåŠ¨ä½œï¼Œæ— æ³•æŒ½å›
                                episode['status'] = 'terminated'
                                terminated_episodes.append(episode)
                                completed_episodes.append(episode)
                            else:
                                # çŠ¶æ€2: ç”Ÿæˆå®Œæˆï¼Œåˆ°æœ«å°¾è¡¨è¾¾å¼
                                episode['status'] = 'waiting_final'
                                waiting_final.append(episode)
                        elif info.get('waiting_for_ic'):
                            # çŠ¶æ€3: ç”Ÿæˆåˆ°ä¸­é—´è¡¨è¾¾å¼
                            episode['status'] = 'waiting_intermediate'
                            waiting_intermediate.append(episode)

                        # æ£€æŸ¥episodeé•¿åº¦é™åˆ¶
                        if episode['steps'] >= self.max_episode_steps:
                            episode['status'] = 'terminated'
                            terminated_episodes.append(episode)
                            completed_episodes.append(episode)

                    except Exception as e:
                        print(f"âŒ Error in episode {episode['env_idx']}: {e}")
                        episode['return'] -= 10.0
                        episode['status'] = 'terminated'
                        terminated_episodes.append(episode)
                        completed_episodes.append(episode)

            # é˜¶æ®µ2: å¤„ç†çŠ¶æ€1ï¼ˆç›´æ¥ç»“æŸï¼‰å’Œæ”¶é›†ç­‰å¾…çŠ¶æ€
            for ep in active_episodes[:]:  # å¤åˆ¶åˆ—è¡¨ä»¥ä¾¿ä¿®æ”¹
                if ep['status'] == 'terminated':
                    terminated_episodes.append(ep)
                    active_episodes.remove(ep)
                elif ep['status'] == 'waiting_final':
                    # çŠ¶æ€2ï¼šæœ«å°¾è¡¨è¾¾å¼ï¼Œç§»åˆ°waiting_finalåˆ—è¡¨ï¼Œä¸å‚ä¸æœ¬è½®åŒæ­¥
                    waiting_final.append(ep)
                    active_episodes.remove(ep)

            # é˜¶æ®µ3: æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‰©ä½™æ´»è·ƒepisodeséƒ½åˆ°è¾¾çŠ¶æ€3åŒæ­¥ç‚¹
            if active_episodes and all(ep['status'] == 'waiting_intermediate' for ep in active_episodes):
                # é˜¶æ®µ4: æ‰¹å¤„ç†çŠ¶æ€3ï¼ˆä¸­é—´è¡¨è¾¾å¼ï¼‰
                intermediate_expressions = []
                for ep in active_episodes:
                    try:
                        expr = ep['env'].get_current_expression()
                        if expr:
                            intermediate_expressions.append((ep, expr))
                    except:
                        continue

                if intermediate_expressions:
                    # æ‰¹é‡è®¡ç®—ä¸­é—´IC
                    ic_results = self.batch_calculate_intermediate_ic(
                        [expr for ep, expr in intermediate_expressions]
                    )

                    # åˆ†é…ç»“æœå¹¶æ¢å¤episodesåˆ°è¿è¡ŒçŠ¶æ€
                    for (ep, expr), ic in zip(intermediate_expressions, ic_results):
                        ep['env'].receive_ic_result(ic)
                        ep['status'] = 'running'

                        # è¡¥å…¨å¯¹åº”çš„pending transitions
                        self._complete_pending_transitions(ep, ic, 'waiting_intermediate')

            # é˜¶æ®µ5: å¤„ç†å·²å®Œæˆçš„æœ«å°¾è¡¨è¾¾å¼episodes
            # æ³¨æ„ï¼šçŠ¶æ€2çš„episodesç”±å¦ä¸€å¥—å¼‚æ­¥ç³»ç»Ÿå¤„ç†ï¼Œä¸åœ¨è¿™é‡Œç«‹å³å¤„ç†

            # æ›´æ–°æ´»è·ƒepisodesåˆ—è¡¨ï¼ˆåªæœ‰çŠ¶æ€ä¸º'running'çš„ï¼‰
            active_episodes = [ep for ep in active_episodes if ep['status'] == 'running']

            # é˜¶æ®µ6: æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†å¾…å®šçš„çŠ¶æ€2è¡¨è¾¾å¼
            if len(self.pending_final_expressions) >= self.batch_final_threshold:
                self._process_pending_final_expressions()

            # å¤„ç†å·²å®Œæˆçš„episodes
            for episode in completed_episodes:
                self.episodes += 1
                self.train_return.append(episode['return'])

                if self.episodes % self.log_interval == 0:
                    try:
                        self.writer.add_scalar(
                            'ic/train', self.env.env.pool.state['best_ic_ret'], self.steps)
                    except Exception as e:
                        print(f"Warning: Failed to log training stats: {e}")

        # æ‰¹æ¬¡è®­ç»ƒé—´éš”å¤„ç†
        self.train_step_interval()

    def explore_for_env(self, env):
        """ä¸ºç‰¹å®šç¯å¢ƒé€‰æ‹©éšæœºåŠ¨ä½œ"""
        allowed_action = env.action_masks()

        if hasattr(env, 'sep_action') and env.sep_action is not None:
            sep_idx = env.sep_action
            if allowed_action[sep_idx]:
                if np.random.rand() < 0.3:
                    return sep_idx

        action = env.action_space.sample()
        while not allowed_action[action]:
            action = env.action_space.sample()
        return action

    def exploit_for_env(self, env, state):
        """ä¸ºç‰¹å®šç¯å¢ƒé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ"""
        state = torch.ByteTensor(state).unsqueeze(0).to(self.device).float()
        with torch.no_grad():
            q_values = self.online_net.calculate_q(states=state)
            forbid_action = torch.BoolTensor(~env.action_masks()).to(self.device)
            q_values[:, forbid_action] = -1e6
            action = q_values.argmax().item()
        return action

    def batch_calculate_intermediate_ic(self, expressions):
        """æ‰¹é‡è®¡ç®—ä¸­é—´è¡¨è¾¾å¼çš„IC"""
        print(f"ğŸ”„ Batch calculating IC for {len(expressions)} intermediate expressions")

        ic_results = []
        for expr in expressions:
            try:
                # è¿™é‡Œè°ƒç”¨å•ä¸ªICè®¡ç®—ï¼Œå®é™…å®ç°ä¸­åº”è¯¥æ‰¹é‡åŒ–
                ic = self.env.env.pool.calculate_single_ic_for_expr(expr)
                ic_results.append(ic)
            except Exception as e:
                print(f"âŒ Failed to calculate IC for intermediate expression: {e}")
                ic_results.append(0.0)

        return ic_results

    def _process_pending_final_expressions(self):
        """å¤„ç†å¾…å®šçš„çŠ¶æ€2è¡¨è¾¾å¼æ‰¹å¤„ç†"""
        if not self.pending_final_expressions:
            return

        print(f"ğŸ”„ Processing {len(self.pending_final_expressions)} pending final expressions")

        # æå–è¡¨è¾¾å¼åˆ—è¡¨
        expressions = [expr for ep, expr in self.pending_final_expressions]

        # æ‰¹é‡è®¡ç®—å› å­å€¼ï¼ˆå¼‚æ­¥ï¼Œé¿å…æ¯ä¸ªè¡¨è¾¾å¼éƒ½è°ƒç”¨Lorentzï¼‰
        try:
            print(f"ğŸ”¬ Starting batch Lorentz computation for {len(expressions)} expressions...")

            # è°ƒç”¨æ‰¹é‡å› å­è®¡ç®—API
            from external_compute import compute_batch_factor_values
            results = compute_batch_factor_values(expressions)

            # éªŒè¯æ‰€æœ‰è¡¨è¾¾å¼éƒ½å·²è®¡ç®—å®Œæˆ
            computed_count = 0
            for expr in expressions:
                expr_str = str(expr)
                if expr_str in results and results[expr_str][0] is not None:
                    computed_count += 1
                else:
                    print(f"âš ï¸ Expression not computed: {expr_str}")

            print(f"âœ… Batch computation completed: {computed_count}/{len(expressions)} expressions successful")

            if computed_count == 0:
                raise RuntimeError("All expressions failed to compute")

        except Exception as e:
            print(f"âŒ Batch factor computation failed: {str(e)}")
            print("ğŸ“‹ Failed expressions:")
            for i, expr in enumerate(expressions, 1):
                print(f"   {i}. {str(expr)}")
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise

        # æ‰¹é‡äº¤ç»™å››æ± ç³»ç»Ÿå¤„ç†
        self.batch_calculate_final_ic(expressions)

        # æ¸…ç©ºå¾…å¤„ç†é˜Ÿåˆ—
        self.pending_final_expressions.clear()
        print(f"âœ… Processed {len(expressions)} final expressions")

    def batch_calculate_final_ic(self, expressions):
        """æ‰¹é‡è®¡ç®—æœ«å°¾è¡¨è¾¾å¼çš„ICï¼ˆäº¤ç»™å››æ± ç³»ç»Ÿï¼‰"""
        print(f"ğŸ”„ Batch calculating final IC for {len(expressions)} expressions")

        for expr in expressions:
            try:
                # äº¤ç»™ç°æœ‰çš„pool.try_new_exprå¤„ç†ï¼ˆå››æ± ç³»ç»Ÿï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œçš„rewardæ˜¯å››æ± ç³»ç»Ÿè®¡ç®—çš„å»¶è¿Ÿreward
                reward = self.env.env.pool.try_new_expr(expr)

                # æ‰¾åˆ°å¯¹åº”çš„episodeå¹¶è¡¥å…¨å…¶pending transitions
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ¯ä¸ªè¡¨è¾¾å¼å¯¹åº”ä¸€ä¸ªepisode
                # å®é™…ä¸Šåº”è¯¥é€šè¿‡æ›´ç²¾ç¡®çš„åŒ¹é…æœºåˆ¶
                for ep, ep_expr in self.pending_final_expressions:
                    if str(ep_expr) == str(expr):
                        self._complete_pending_transitions(ep, reward, 'waiting_final')
                        break

            except Exception as e:
                print(f"âŒ Failed to process final expression: {e}")

    def train_step_interval(self):
        """æ‰¹æ¬¡è®­ç»ƒé—´éš”å¤„ç†"""
        self.epsilon_train.step()

        if self.steps % self.target_update_interval == 0:
            self.update_target()

        if self.is_update():
            self.learn()

        if (self.steps % self.eval_interval == 0) and (len(self.env.env.pool.state['exprs']) >= 1):
            self.evaluate()
            self.save_models(os.path.join(self.model_dir, 'final'))

