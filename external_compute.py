#!/usr/bin/env python3
"""
External Factor Computation Script
ä½¿ç”¨Lorentzç¨‹åºè®¡ç®—alphaå› å­å€¼å¹¶è¾“å‡ºCSVæ ¼å¼ç»“æœ
"""

import sys
import os
import json
import subprocess
import tempfile
import logging
from pathlib import Path
from typing import Tuple, Dict, List, Optional
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LorentzConfig:
    """Lorentzé…ç½®ç®¡ç†"""

    def __init__(self):
        # å°è¯•åŠ è½½AlphaQCMé…ç½®
        try:
            from config_loader import load_config_for_external_compute
            alphaqcm_config = load_config_for_external_compute()
            data_config = alphaqcm_config.get_data_config()
            lorentz_config = alphaqcm_config.get_lorentz_config()
            paths_config = alphaqcm_config.get_paths_config()

            # ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°
            self.lorentz_executable = lorentz_config.get('executable_path', '/dfs/dataset/365-1734663142170/data/Lorentz_History-Insider')
            self.thread_num = lorentz_config.get('thread_num', 8)
            self.start_date = data_config.get('start_date', '20200101')
            self.end_date = data_config.get('end_date', '20241231')
            self.frequency_config = data_config.get('frequency_config', '1dper1d')

            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„è®¾ç½®
            self.output_factor_root_dir = paths_config.get('factors_output_dir', '/dfs/data/Factors')
            self.output_abnormal_root_dir = paths_config.get('abnormal_stats_dir', '/dfs/data/AbnormalStats')

        except Exception:
            # å¦‚æœé…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
            self.lorentz_executable = os.getenv('LORENTZ_EXECUTABLE', '/dfs/dataset/365-1734663142170/data/Lorentz_History-Insider')
            self.thread_num = int(os.getenv('THREAD_NUM', '8'))
            self.start_date = os.getenv('START_DATE', '20200101')
            self.end_date = os.getenv('END_DATE', '20241231')
            self.frequency_config = os.getenv('FREQUENCY_CONFIG', '1dper1d')
            self.output_factor_root_dir = os.getenv('OUTPUT_FACTOR_ROOT_DIR', '/dfs/data/Factors')
            self.output_abnormal_root_dir = os.getenv('OUTPUT_ABNORMAL_ROOT_DIR', '/dfs/data/AbnormalStats')

        # åŸºäºé¢‘ç‡é…ç½®åŠ¨æ€ç”Ÿæˆè·¯å¾„
        base_data_path = '/dfs/dataset/365-1734663142170/data'

        # åŠ¨æ€ç”Ÿæˆè·¯å¾„ï¼ˆå¦‚æœæ²¡æœ‰åœ¨é…ç½®ä¸­æŒ‡å®šï¼‰
        self.interval_json = os.getenv('INTERVAL_JSON',
            f'{base_data_path}/LorentzConfigTemplate/{self.frequency_config}/interval_{self.frequency_config}.json')
        self.data_root_dir = os.getenv('DATA_ROOT_DIR',
            f'{base_data_path}/BasicFieldsDump-Latest-Release/{self.frequency_config}')
        self.daily_data_dir = os.getenv('DAILY_DATA_DIR',
            f'{base_data_path}/BasicFieldsDump-Latest-Release/{self.frequency_config}')

        # ç¡®ä¿è¾“å‡ºç›®å½•åŒ…å«é¢‘ç‡é…ç½®
        if not self.output_factor_root_dir.endswith(self.frequency_config):
            self.output_factor_root_dir = os.path.join(self.output_factor_root_dir, self.frequency_config)
        if not self.output_abnormal_root_dir.endswith(self.frequency_config):
            self.output_abnormal_root_dir = os.path.join(self.output_abnormal_root_dir, self.frequency_config)


class LorentzExecutor:
    """Lorentzç¨‹åºæ‰§è¡Œå™¨"""

    def __init__(self, config: LorentzConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def execute_for_date(self, date_str: str, factor_json_path: str, output_names_path: str,
                        output_module_name: str, load_prev_days: int = 1) -> Tuple[bool, str]:
        """
        ä¸ºæŒ‡å®šæ—¥æœŸæ‰§è¡ŒLorentzè®¡ç®—

        Args:
            date_str: è®¡ç®—æ—¥æœŸ (YYYYMMDDæ ¼å¼)
            factor_json_path: å› å­JSONé…ç½®æ–‡ä»¶è·¯å¾„
            output_names_path: è¾“å‡ºå› å­åç§°æ–‡ä»¶è·¯å¾„
            output_module_name: è¾“å‡ºæ¨¡å—åç§°
            load_prev_days: åŠ è½½å‰Nå¤©çš„å‚æ•°

        Returns:
            Tuple of (success, error_message)
        """
        try:
            # ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶ - ä¿å­˜åˆ°debugç›®å½•ä»¥ä¾¿æ£€æŸ¥
            cfg_content = self._generate_cfg_content(
                date_str, factor_json_path, output_names_path, output_module_name, load_prev_days
            )

            # åˆ›å»ºdebugç›®å½•
            debug_dir = os.path.join(os.getcwd(), 'lorentz_debug')
            os.makedirs(debug_dir, exist_ok=True)

            cfg_file_path = os.path.join(debug_dir, f'lorentz_config_{date_str}.cfg')
            with open(cfg_file_path, 'w') as cfg_file:
                cfg_file.write(cfg_content)

            try:
                # æ‰§è¡ŒLorentzç¨‹åº
                cmd = [self.config.lorentz_executable, cfg_file_path]

                # ç®€åŒ–çš„å¼€å§‹æ ‡è®°
                print(f"\n=== LORENTZ START ({date_str}) ===", file=sys.stderr)

                # ä½¿ç”¨os.systemç¡®ä¿è¾“å‡ºå¯è§
                import os
                cmd_str = ' '.join(cmd)
                return_code = os.system(cmd_str)

                # ç®€åŒ–çš„ç»“æŸæ ‡è®°
                print(f"=== LORENTZ END (code: {return_code}) ===\n", file=sys.stderr)

                # æ¨¡æ‹Ÿsubprocess.CompletedProcess
                class MockCompletedProcess:
                    def __init__(self, returncode):
                        self.returncode = returncode
                        self.stdout = ""
                        self.stderr = ""

                result = MockCompletedProcess(return_code)

                # ä¸ºäº†å‘åå…¼å®¹ï¼Œè®¾ç½®ç©ºçš„stdout/stderr
                result.stdout = ""
                result.stderr = ""

                if result.returncode == 0:
                    self.logger.info(f"Lorentz execution completed successfully for {date_str}")
                    return True, ""
                else:
                    error_msg = f"Lorentz failed with return code {result.returncode}: {result.stderr}"
                    self.logger.error(error_msg)
                    return False, error_msg

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(cfg_file_path)
                except:
                    pass

        except subprocess.TimeoutExpired:
            error_msg = f"Lorentz execution timed out for {date_str}"
            self.logger.error(error_msg)
            return False, error_msg
        except Exception as e:
            error_msg = f"Failed to execute Lorentz for {date_str}: {e}"
            self.logger.error(error_msg)
            return False, error_msg

    def _generate_cfg_content(self, date_str: str, factor_json_path: str,
                            output_names_path: str, output_module_name: str, load_prev_days: int = 1) -> str:
        """ç”ŸæˆLorentzé…ç½®æ–‡ä»¶å†…å®¹"""
        cfg_lines = [
            f"DATE={date_str}",
            f"INTERVAL_JSON={self.config.interval_json}",
            "",
            "[BasicFields]",
            f"DATA_ROOT_DIR={self.config.data_root_dir}",
            f"LOAD_PREV_DAYS={load_prev_days}",
            f"THREAD_NUM={self.config.thread_num}",
            f"AUTO_PROD_CO_DEPENDENCY=TRUE",
            f"DAILY_DATA_DIR={self.config.daily_data_dir}",
            "",
            "[ComputeGraph]",
            f"THREAD_NUM={self.config.thread_num}",
            f"FACTOR_JSON={factor_json_path}",
            f"OUTPUT_MODULE_NAME={output_module_name}",
            f"OUTPUTS_CONFIG_FILES={output_names_path}",
            f"EMABLE_OUTPUT_CSV=TRUE",
            f"CSV_FLOAT_PRECISION=6",
            f"OUTPUT_FACTOR_ROOT_DIR={self.config.output_factor_root_dir}",
            f"OUTPUT_ABNORMAL_ROOT_DIR={self.config.output_abnormal_root_dir}",
        ]
        return "\n".join(cfg_lines)


class LorentzResultParser:
    """Lorentzç»“æœè§£æå™¨"""

    def __init__(self, config: LorentzConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)

    def _hhmmss_to_minutecode(self, hhmmss: str) -> int:
        """
        å°†HHMMSSæ ¼å¼è½¬æ¢ä¸ºminuteCode (0-240)

        Args:
            hhmmss: æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œå¦‚ "093000"

        Returns:
            minuteCode: 0-240çš„åˆ†é’Ÿç¼–ç 
        """
        try:
            # è§£æå°æ—¶å’Œåˆ†é’Ÿ
            hour = int(hhmmss[:2])
            minute = int(hhmmss[2:4])

            # è®¡ç®—ä»9:30å¼€å§‹çš„åˆ†é’Ÿæ•°
            # 9:30 = 0åˆ†é’Ÿ
            # 11:30 = 120åˆ†é’Ÿ (ä¸­åˆä¼‘æ¯)
            # 13:00 = 121åˆ†é’Ÿ (ä¸‹åˆå¼€ç›˜)
            # 15:00 = 240åˆ†é’Ÿ (æ”¶ç›˜)

            if hour < 12:  # ä¸Šåˆ
                total_minutes = (hour - 9) * 60 + minute - 30
            else:  # ä¸‹åˆ
                # ä¸‹åˆ1:00å¼€å§‹ï¼Œå‡å»ä¸­åˆä¼‘æ¯æ—¶é—´
                total_minutes = 120 + (hour - 13) * 60 + minute

            return int(total_minutes)

        except (ValueError, IndexError):
            self.logger.error(f"Invalid HHMMSS format: {hhmmss}")
            return -1

    def parse_factor_output(self, date_str: str, factor_name: str) -> Optional[pd.DataFrame]:
        """
        è§£ææŒ‡å®šæ—¥æœŸå’Œå› å­çš„æ‰€æœ‰è¾“å‡ºæ–‡ä»¶

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)
            factor_name: å› å­åç§°

        Returns:
            åŒ…å«symbol, minuteCode, factor_valueçš„DataFrameï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            # æ ¼å¼: /output_root/AutoML/YYYY/YYYYMMDD/eHHMMSS.csv
            year = date_str[:4]
            output_pattern = os.path.join(
                self.config.output_factor_root_dir,
                "AutoML",
                year,
                date_str,
                "e*.csv"  # åŒ¹é…æ‰€æœ‰eå¼€å¤´çš„æ—¶é—´æˆ³æ–‡ä»¶
            )

            import glob
            output_files = glob.glob(output_pattern)

            if not output_files:
                self.logger.warning(f"No output files found for {date_str}")
                return None

            # è¯»å–æ‰€æœ‰æ—¶é—´ç‚¹çš„æ–‡ä»¶
            all_results = []

            for file_path in sorted(output_files):  # æŒ‰æ—¶é—´æ’åº
                try:
                    # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
                    filename = os.path.basename(file_path)
                    if not filename.startswith('e') or not filename.endswith('.csv'):
                        continue

                    hhmmss = filename[1:-4]  # å»æ‰'e'å’Œ'.csv'
                    minute_code = self._hhmmss_to_minutecode(hhmmss)

                    if minute_code < 0:
                        continue

                    # è¯»å–CSVæ–‡ä»¶
                    df = pd.read_csv(file_path)

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€éœ€çš„å› å­åˆ—
                    if factor_name not in df.columns:
                        self.logger.warning(f"Factor {factor_name} not found in {file_path}")
                        continue

                    # æå–æ•°æ®
                    temp_df = df[['symbol', factor_name]].copy()
                    temp_df['date'] = pd.to_datetime(date_str, format='%Y%m%d')
                    temp_df['minuteCode'] = minute_code
                    temp_df = temp_df.rename(columns={factor_name: 'factor_value'})

                    all_results.append(temp_df)

                except Exception as e:
                    self.logger.warning(f"Error reading {file_path}: {e}")
                    continue

            if not all_results:
                self.logger.error(f"No valid factor data found for {date_str}")
                return None

            # åˆå¹¶æ‰€æœ‰æ—¶é—´ç‚¹çš„æ•°æ®
            combined_df = pd.concat(all_results, ignore_index=True)

            # è®°å½•æ•°æ®ç»Ÿè®¡ä¿¡æ¯
            actual_minute_codes = sorted(combined_df['minuteCode'].unique())
            n_time_points = len(actual_minute_codes)

            self.logger.info(f"Parsed {len(combined_df)} factor values for {date_str} across {n_time_points} time points: {actual_minute_codes[:5]}...{actual_minute_codes[-5:]}")

            # å¯¹äºper1ä»¥å¤–çš„é…ç½®ï¼Œæ£€æŸ¥æ—¶é—´ç‚¹é—´éš”æ˜¯å¦åˆç†
            if n_time_points > 1:
                intervals = np.diff(actual_minute_codes)
                avg_interval = np.mean(intervals)
                self.logger.info(f"Average time interval: {avg_interval:.1f} minutes")

            return combined_df

        except Exception as e:
            self.logger.error(f"Error parsing output for {date_str}, factor {factor_name}: {e}")
            return None

    def parse_batch_factor_output(self, date_str: str, factor_names: List[str]) -> Optional[Dict[str, pd.DataFrame]]:
        """
        è§£ææ‰¹é‡å› å­è¾“å‡ºï¼Œè¿”å›æ‰€æœ‰å› å­çš„ç»“æœ

        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)
            factor_names: å› å­åç§°åˆ—è¡¨

        Returns:
            å­—å…¸ï¼šå› å­åç§° -> åŒ…å«æ•°æ®çš„DataFrame
        """
        try:
            # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            year = date_str[:4]
            output_pattern = os.path.join(
                self.config.output_factor_root_dir,
                "AutoML",
                year,
                date_str,
                "e*.csv"
            )

            import glob
            output_files = glob.glob(output_pattern)

            if not output_files:
                self.logger.warning(f"No output files found for {date_str}")
                return None

            # ä¸ºæ¯ä¸ªå› å­æ”¶é›†æ•°æ®
            factor_results = {name: [] for name in factor_names}

            # è¯»å–æ‰€æœ‰æ—¶é—´ç‚¹çš„æ–‡ä»¶
            for file_path in sorted(output_files):
                try:
                    # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
                    filename = os.path.basename(file_path)
                    if not filename.startswith('e') or not filename.endswith('.csv'):
                        continue

                    hhmmss = filename[1:-4]
                    minute_code = self._hhmmss_to_minutecode(hhmmss)

                    if minute_code < 0:
                        continue

                    # è¯»å–CSVæ–‡ä»¶
                    df = pd.read_csv(file_path)

                    # ä¸ºæ¯ä¸ªå› å­æå–æ•°æ®
                    for factor_name in factor_names:
                        if factor_name in df.columns:
                            temp_df = df[['symbol', factor_name]].copy()
                            temp_df['date'] = pd.to_datetime(date_str, format='%Y%m%d')
                            temp_df['minuteCode'] = minute_code
                            temp_df = temp_df.rename(columns={factor_name: 'factor_value'})

                            factor_results[factor_name].append(temp_df)

                except Exception as e:
                    self.logger.warning(f"Error reading {file_path}: {e}")
                    continue

            # åˆå¹¶æ¯ä¸ªå› å­çš„æ•°æ®
            result = {}
            for factor_name, dfs in factor_results.items():
                if dfs:
                    combined_df = pd.concat(dfs, ignore_index=True)
                    result[factor_name] = combined_df
                else:
                    self.logger.warning(f"No data found for factor {factor_name}")

            if result:
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                total_records = sum(len(df) for df in result.values())
                self.logger.info(f"Parsed {total_records} records for {len(result)} factors on {date_str}")
                return result
            else:
                self.logger.error(f"No valid factor data found for {date_str}")
                return None

        except Exception as e:
            self.logger.error(f"Error parsing batch output for {date_str}: {e}")
            return None


def convert_compact_operators_to_lorentz(expr_str: str) -> str:
    """
    å°†å†™æ­»å‚æ•°çš„ç®—å­è½¬æ¢å›Lorentzèƒ½ç†è§£çš„åŸå§‹æ ¼å¼

    Args:
        expr_str: åŒ…å«å†™æ­»å‚æ•°ç®—å­çš„è¡¨è¾¾å¼å­—ç¬¦ä¸²

    Returns:
        è½¬æ¢åçš„è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œç®—å­æ¢å¤ä¸ºåŸå§‹æ ¼å¼
    """
    import re

    def replace_compact_operator(match):
        """æ›¿æ¢å•ä¸ªå†™æ­»å‚æ•°ç®—å­"""
        compact_op = match.group(0)

        # é¦–å…ˆå¤„ç†Tsæ—¶åºç®—å­
        pattern_ts = r'^Ts(\w+)(\d+)([FT])$'
        match_ts = re.match(pattern_ts, compact_op)

        if match_ts:
            op_name = match_ts.group(1)  # åŸºç¡€æ“ä½œå
            window = int(match_ts.group(2))  # çª—å£å¤§å°
            bias_flag = match_ts.group(3)  # Fæˆ–T

            # è½¬æ¢bias
            bias = False if bias_flag == 'F' else True

            # æ„å»ºåŸå§‹æ ¼å¼ï¼šOpName(x, window, bias)
            return f"Ts{op_name}(x, {window}, {str(bias).lower()})"

        # å¤„ç†CsWinsorizeç®—å­
        pattern_winsorize = r'^CsWinsorize(\d+)$'
        match_winsorize = re.match(pattern_winsorize, compact_op)

        if match_winsorize:
            std_ratio = int(match_winsorize.group(1)) / 10  # 05->0.5, 10->1.0, etc.
            return f"CsWinsorize(x, {std_ratio}, group)"

        # å¤„ç†CsRangeMaskç®—å­
        pattern_range = r'^CsRangeMask([LUD])([KR])(\d+)$'
        match_range = re.match(pattern_range, compact_op)

        if match_range:
            border = match_range.group(1)  # L/U/D
            op_type = match_range.group(2)  # K/R
            pct = int(match_range.group(3))  # 01/05/10/25

            if border == 'L':  # Lowerè¾¹
                if op_type == 'K':  # è¦æå€¼
                    lower_pct, upper_pct = 0, pct
                else:  # å»æå€¼
                    lower_pct, upper_pct = pct, 100
            elif border == 'U':  # Upperè¾¹
                if op_type == 'K':  # è¦æå€¼
                    lower_pct, upper_pct = 100 - pct, 100
                else:  # å»æå€¼
                    lower_pct, upper_pct = 0, 100 - pct
            else:  # åŒè¾¹ D
                if op_type == 'K':  # è¦æå€¼
                    lower_pct, upper_pct = pct, 100 - pct
                else:  # å»æå€¼
                    # å¯¹äºåŒè¾¹å»æå€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å•ä¸ªèŒƒå›´è¡¨ç¤ºï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                    lower_pct, upper_pct = 0, pct

            return f"CsRangeMask(x, {lower_pct}, {upper_pct}, substitute, mask, group)"

        return compact_op

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰å†™æ­»å‚æ•°ç®—å­
    # åŒ¹é…å„ç§å†™æ­»å‚æ•°ç®—å­
    patterns = [
        r'\bTs\w+\d+[FT]\b',        # Tså¼€å¤´çš„æ—¶åºç®—å­
        r'\bCsWinsorize\d+\b',      # CsWinsorizeç®—å­
        r'\bCsRangeMask\w+\d+\b',   # CsRangeMaskç®—å­
    ]

    result = expr_str
    for pattern in patterns:
        result = re.sub(pattern, replace_compact_operator, result)

    return result


def parse_alpha_expression(expr_str: str) -> Dict[str, str]:
    """
    è§£æalphaè¡¨è¾¾å¼å¹¶ç”ŸæˆLorentzé…ç½®
    å°†å†™æ­»å‚æ•°çš„ç®—å­è½¬æ¢å›Lorentzèƒ½ç†è§£çš„åŸå§‹æ ¼å¼

    Args:
        expr_str: Alphaè¡¨è¾¾å¼å­—ç¬¦ä¸²

    Returns:
        åŒ…å«å› å­åç§°å’Œè¡¨è¾¾å¼çš„å­—å…¸
    """
    # å°†å†™æ­»å‚æ•°çš„ç®—å­è½¬æ¢å›åŸå§‹æ ¼å¼
    converted_expr_str = convert_compact_operators_to_lorentz(expr_str)

    # ç”Ÿæˆå› å­åç§° (ä½¿ç”¨è¡¨è¾¾å¼å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†)
    import hashlib
    factor_name = f"Factor_{hashlib.md5(converted_expr_str.encode()).hexdigest()[:8]}"

    return {
        "factor_name": factor_name,
        "expression": converted_expr_str
    }


def convert_field_references(expr_str: str) -> str:
    """
    è½¬æ¢è¡¨è¾¾å¼ä¸­çš„å­—æ®µå¼•ç”¨ï¼Œæ·»åŠ @å‰ç¼€

    Args:
        expr_str: åŸå§‹è¡¨è¾¾å¼å­—ç¬¦ä¸²

    Returns:
        è½¬æ¢åçš„è¡¨è¾¾å¼å­—ç¬¦ä¸²
    """
    # åŒ¹é…å­—æ®µå¼•ç”¨æ¨¡å¼ï¼Œå¦‚ $Slice.LastPrice, $Preload.Volume ç­‰
    import re

    def replace_field(match):
        field_ref = match.group(0)
        # å»æ‰å¼€å¤´çš„$ï¼ŒåŠ ä¸Š@
        return '@' + field_ref[1:]

    # åŒ¹é… $åè·Ÿå­—æ¯çš„å­—æ®µå¼•ç”¨
    pattern = r'\$[A-Za-z][A-Za-z0-9_.]*'
    return re.sub(pattern, replace_field, expr_str)


def analyze_lookback_requirements(expr_str: str) -> Dict[str, int]:
    """
    åˆ†æè¡¨è¾¾å¼ä¸­çš„Tsç®—å­ï¼Œè®¡ç®—å›çœ‹æ—¶é—´è¦æ±‚

    Args:
        expr_str: è¡¨è¾¾å¼å­—ç¬¦ä¸²

    Returns:
        åŒ…å«rolling_prev_dayså’Œrolling_prev_intervalsçš„å­—å…¸
    """
    import re

    rolling_prev_days = 0
    rolling_prev_intervals = 0

    # ä½¿ç”¨æ ˆæ¥è§£æåµŒå¥—çš„å‡½æ•°è°ƒç”¨
    def parse_function_calls(text: str):
        """è§£ææ‰€æœ‰å‡½æ•°è°ƒç”¨ï¼Œè¿”å›(func_name, args_str)åˆ—è¡¨"""
        calls = []
        i = 0
        while i < len(text):
            # æŸ¥æ‰¾å‡½æ•°å
            if text[i].isalpha():
                # æ‰¾åˆ°å‡½æ•°åå¼€å§‹
                start = i
                while i < len(text) and (text[i].isalnum() or text[i] == '_'):
                    i += 1
                func_name = text[start:i]

                # æŸ¥æ‰¾å¯¹åº”çš„å·¦æ‹¬å·
                while i < len(text) and text[i] != '(':
                    i += 1

                if i < len(text) and text[i] == '(':
                    # æ‰¾åˆ°å·¦æ‹¬å·ï¼Œå¼€å§‹è§£æå‚æ•°
                    paren_count = 1
                    arg_start = i + 1
                    i += 1

                    while i < len(text) and paren_count > 0:
                        if text[i] == '(':
                            paren_count += 1
                        elif text[i] == ')':
                            paren_count -= 1
                        i += 1

                    if paren_count == 0:  # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·
                        args_str = text[arg_start:i-1]
                        calls.append((func_name, args_str))

                        # é€’å½’è§£æå‚æ•°ä¸­çš„åµŒå¥—è°ƒç”¨
                        calls.extend(parse_function_calls(args_str))
            else:
                i += 1

        return calls

    # è§£ææ‰€æœ‰å‡½æ•°è°ƒç”¨
    all_calls = parse_function_calls(expr_str)

    # åªå¤„ç†Tså¼€å¤´çš„ç®—å­
    ts_calls = [(name, args) for name, args in all_calls if name.startswith('Ts')]

    for func_name, args_str in ts_calls:
        # è§£æå‚æ•°
        args = [arg.strip() for arg in args_str.split(',') if arg.strip()]

        # éœ€è¦è‡³å°‘3ä¸ªå‚æ•°
        if len(args) < 3:
            continue

        try:
            # å€’æ•°ç¬¬äºŒä¸ªå‚æ•°ï¼šæ—¶é—´çª—å£
            time_window = int(args[-2])

            # æœ€åä¸€ä¸ªå‚æ•°ï¼šæ˜¯å¦è·¨æ—¥
            last_arg = args[-1].lower().strip()
            is_cross_day = last_arg in ['true', '1', 'yes']

            if is_cross_day:
                # è·¨æ—¥ï¼šä½¿ç”¨rolling_prev_days
                rolling_prev_days = max(rolling_prev_days, time_window)
            else:
                # åŒæ—¥ï¼šä½¿ç”¨rolling_prev_intervals
                rolling_prev_intervals = max(rolling_prev_intervals, time_window)

        except (ValueError, IndexError):
            # å‚æ•°è§£æå¤±è´¥ï¼Œè·³è¿‡
            continue

    # æ„å»ºç»“æœï¼šrolling_prev_daysä¼˜å…ˆçº§é«˜äºrolling_prev_intervals
    result = {}
    if rolling_prev_days > 0:
        result['rolling_prev_days'] = rolling_prev_days
    elif rolling_prev_intervals > 0:
        result['rolling_prev_intervals'] = rolling_prev_intervals

    return result


def parse_expression_with_intermediates(expr_str: str) -> Dict[str, any]:
    """
    è§£æè¡¨è¾¾å¼ï¼Œæ”¯æŒä¸­é—´å˜é‡æå–
    ä¸¥æ ¼æŒ‰ç…§Lorentzè§„åˆ™ï¼šCsç®—å­åªèƒ½åœ¨cross_sectionä¸­ï¼Œä¸”å‚æ•°åªèƒ½æ˜¯ç®€å•å¼•ç”¨

    Args:
        expr_str: åŸå§‹è¡¨è¾¾å¼å­—ç¬¦ä¸²

    Returns:
        åŒ…å«slice_expressions, cross_section_expressions, final_expressionçš„å­—å…¸
    """
    import re

    # ç”¨äºç”Ÿæˆå”¯ä¸€å˜é‡åçš„è®¡æ•°å™¨
    var_counter = 0

    def get_next_var_name():
        nonlocal var_counter
        var_counter += 1
        return f"var_{var_counter}"

    # å­˜å‚¨ä¸­é—´å˜é‡å®šä¹‰
    slice_intermediates = []  # sliceä¸­çš„ä¸­é—´å˜é‡ï¼ˆæ— Csç®—å­ï¼‰
    cross_section_intermediates = []  # cross_sectionä¸­çš„ä¸­é—´å˜é‡ï¼ˆå¯èƒ½åŒ…å«Csç®—å­ï¼‰

    def is_simple_arg(expr: str) -> bool:
        """
        åˆ¤æ–­Csç®—å­å‚æ•°æ˜¯å¦ä¸ºç®€å•å‚æ•°
        Csç®—å­å‚æ•°åªèƒ½æ˜¯ï¼šå˜é‡å¼•ç”¨ã€å­—æ®µå¼•ç”¨ã€constå¸¸é‡
        """
        expr = expr.strip()

        # å˜é‡å¼•ç”¨
        if expr.startswith('@'):
            return True

        # å­—æ®µå¼•ç”¨
        if expr.replace('.', '').replace('_', '').isalnum() and '.' in expr:
            return True

        # constå¸¸é‡ï¼ˆæ•°å­—ã€å¸ƒå°”ï¼‰
        try:
            float(expr)
            return True
        except:
            pass

        if expr.lower() in ['true', 'false']:
            return True

        return False

    def extract_subexpressions(text: str) -> str:
        """
        æå–å¤æ‚å­è¡¨è¾¾å¼ï¼Œç¡®ä¿Csç®—å­åŠå…¶å¤æ‚å‚æ•°éƒ½åœ¨cross_sectionä¸­
        """
        result = text

        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ‰€æœ‰éCsç®—å­ï¼Œæå–å¤æ‚å‚æ•°ä¸ºsliceä¸­é—´å˜é‡
        def process_non_cs_functions(expr: str) -> str:
            """å¤„ç†éCsç®—å­ï¼Œæå–å¤æ‚å‚æ•°"""
            res = expr

            # ä½¿ç”¨æ ˆè§£æå‡½æ•°è°ƒç”¨
            def parse_function_calls(e: str):
                calls = []
                i = 0
                while i < len(e):
                    if e[i].isalpha():
                        func_start = i
                        while i < len(e) and (e[i].isalnum() or e[i] == '_'):
                            i += 1

                        func_name = e[func_start:i]

                        if func_name.startswith('Cs'):
                            # è·³è¿‡Csç®—å­ï¼Œåœ¨åç»­å¤„ç†
                            continue

                        if i < len(e) and e[i] == '(':
                            paren_count = 1
                            args_start = i + 1
                            i += 1

                            while i < len(e) and paren_count > 0:
                                if e[i] == '(':
                                    paren_count += 1
                                elif e[i] == ')':
                                    paren_count -= 1
                                i += 1

                            if paren_count == 0:
                                args_str = e[args_start:i-1]

                                # è§£æå‚æ•°
                                args = []
                                current_arg = ""
                                arg_depth = 0

                                for char in args_str:
                                    if char == '(':
                                        arg_depth += 1
                                        current_arg += char
                                    elif char == ')':
                                        arg_depth -= 1
                                        current_arg += char
                                    elif char == ',' and arg_depth == 0:
                                        if current_arg.strip():
                                            args.append(current_arg.strip())
                                        current_arg = ""
                                    else:
                                        current_arg += char

                                if current_arg.strip():
                                    args.append(current_arg.strip())

                                calls.append((func_name, args, func_start, i))
                    else:
                        i += 1
                return calls

            calls = parse_function_calls(res)
            calls.sort(key=lambda x: x[2], reverse=True)  # ä»åå¾€å‰å¤„ç†

            for func_name, args, start_pos, end_pos in calls:
                processed_args = []
                for arg in args:
                    if not is_simple_arg(arg):
                        var_name = get_next_var_name()
                        slice_intermediates.append({
                            'name': var_name,
                            'expression': arg,
                            'output': False
                        })
                        processed_args.append(f'@{var_name}')
                    else:
                        processed_args.append(arg)

                # é‡å»ºè°ƒç”¨
                new_args_str = ','.join(processed_args)
                new_call = f'{func_name}({new_args_str})'
                res = res[:start_pos] + new_call + res[end_pos:]

            return res

        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†éCsç®—å­
        result = process_non_cs_functions(result)

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†Csç®—å­åŠå…¶å¤æ‚å‚æ•°
        def process_cs_functions(expr: str) -> str:
            """å¤„ç†Csç®—å­ï¼Œå°†å…¶å¤æ‚å‚æ•°æå–ä¸ºcross_sectionä¸­é—´å˜é‡"""
            res = expr

            # æŸ¥æ‰¾æ‰€æœ‰Csç®—å­
            cs_pattern = r'\b(Cs\w*)\(([^()]*(?:\([^()]*\)[^()]*)*)\)'
            cs_matches = []

            for match in re.finditer(cs_pattern, res):
                func_name = match.group(1)
                args_str = match.group(2)
                start_pos = match.start()
                end_pos = match.end()

                # è§£æå‚æ•°
                args = []
                current_arg = ""
                paren_depth = 0

                for char in args_str:
                    if char == '(':
                        paren_depth += 1
                        current_arg += char
                    elif char == ')':
                        paren_depth -= 1
                        current_arg += char
                    elif char == ',' and paren_depth == 0:
                        if current_arg.strip():
                            args.append(current_arg.strip())
                        current_arg = ""
                    else:
                        current_arg += char

                if current_arg.strip():
                    args.append(current_arg.strip())

                cs_matches.append((func_name, args, start_pos, end_pos))

            # ä»åå¾€å‰å¤„ç†
            cs_matches.sort(key=lambda x: x[2], reverse=True)

            for func_name, args, start_pos, end_pos in cs_matches:
                processed_args = []
                for arg in args:
                    if not is_simple_arg(arg):
                        # Csç®—å­çš„å¤æ‚å‚æ•°ï¼Œæå–ä¸ºcross_sectionä¸­é—´å˜é‡
                        var_name = get_next_var_name()
                        cross_section_intermediates.append({
                            'name': var_name,
                            'expression': arg,
                            'output': False
                        })
                        processed_args.append(f'@{var_name}')
                    else:
                        processed_args.append(arg)

                # é‡å»ºCsè°ƒç”¨
                new_args_str = ','.join(processed_args)
                new_call = f'{func_name}({new_args_str})'
                res = res[:start_pos] + new_call + res[end_pos:]

            return res

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†Csç®—å­
        result = process_cs_functions(result)

        # ç¬¬ä¸‰æ­¥ï¼šå†æ¬¡å¤„ç†Csç®—å­å†…éƒ¨çš„å¤æ‚å‚æ•°ï¼ˆé€’å½’å¤„ç†ï¼‰
        result = process_cs_functions(result)

        return result

    # ç¬¬ä¸€éï¼šæå–æ‰€æœ‰å¤æ‚å­è¡¨è¾¾å¼åˆ°slice
    processed_expr = extract_subexpressions(expr_str)

    # ç¬¬äºŒéï¼šæ£€æŸ¥æœ€ç»ˆè¡¨è¾¾å¼æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥å¤„ç†
    # å¦‚æœæœ€ç»ˆè¡¨è¾¾å¼åŒ…å«Csç®—å­ä¸”æœ‰å¤æ‚å‚æ•°ï¼Œéœ€è¦å†æ¬¡å¤„ç†
    final_expression = processed_expr
    has_cs_operators = bool(re.search(r'\bCs\w*\(', final_expression))

    return {
        'slice_intermediates': slice_intermediates,
        'cross_section_intermediates': cross_section_intermediates,
        'final_expression': final_expression,
        'has_cs_operators': has_cs_operators
    }


def generate_lorentz_config_files(parsed_expr: Dict[str, str], temp_dir: str) -> Tuple[str, str, str]:
    """
    ç”ŸæˆLorentzéœ€è¦çš„é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒä¸­é—´å˜é‡å’Œcross_section

    Args:
        parsed_expr: è§£æåçš„è¡¨è¾¾å¼ä¿¡æ¯
        temp_dir: ä¸´æ—¶ç›®å½•è·¯å¾„

    Returns:
        Tuple of (factor_json_path, output_names_path, output_module_name)
    """
    factor_name = parsed_expr["factor_name"]
    expression = parsed_expr["expression"]

    # ç¬¬ä¸€æ­¥ï¼šè½¬æ¢å­—æ®µå¼•ç”¨ï¼ˆæ·»åŠ @å‰ç¼€ï¼‰
    converted_expression = convert_field_references(expression)

    # ç¬¬äºŒæ­¥ï¼šè§£æè¡¨è¾¾å¼ï¼Œæå–ä¸­é—´å˜é‡
    parsed_result = parse_expression_with_intermediates(converted_expression)
    slice_intermediates = parsed_result['slice_intermediates']
    cross_section_intermediates = parsed_result['cross_section_intermediates']
    final_expression = parsed_result['final_expression']
    has_cs_operators = parsed_result['has_cs_operators']

    # ç¬¬å››æ­¥ï¼šæ„å»ºé…ç½®ç»“æ„
    default_config = {
        "slice": {
            "trigger": "slice",
            "output": True
        }
    }

    slice_configs = []
    cross_section_configs = []

    # ä¸ºæ¯ä¸ªsliceä¸­é—´å˜é‡å•ç‹¬åˆ†æå›çœ‹æ—¶é—´
    for var_config in slice_intermediates:
        expr = var_config['expression']
        lookback_config = analyze_lookback_requirements(expr)
        if lookback_config:
            var_config.update(lookback_config)
        slice_configs.append(var_config)

    # ä¸ºæ¯ä¸ªcross_sectionä¸­é—´å˜é‡å•ç‹¬åˆ†æå›çœ‹æ—¶é—´
    for var_config in cross_section_intermediates:
        expr = var_config['expression']
        lookback_config = analyze_lookback_requirements(expr)
        if lookback_config:
            var_config.update(lookback_config)
        cross_section_configs.append(var_config)

    # å¤„ç†æœ€ç»ˆè¡¨è¾¾å¼
    if has_cs_operators:
        # æœ‰Csç®—å­ï¼šæ¿€æ´»cross_section
        default_config["cross_section"] = {
            "trigger": "cross_section",
            "output": True
        }

        # æœ€ç»ˆè¡¨è¾¾å¼æ”¾åœ¨cross_sectionä¸­
        final_config = {
            "name": factor_name,
            "expression": final_expression,
            "trigger": "cross_section",
            "output": True
        }

        # ä¸ºæœ€ç»ˆè¡¨è¾¾å¼å•ç‹¬åˆ†æå›çœ‹æ—¶é—´
        final_lookback_config = analyze_lookback_requirements(final_expression)
        if final_lookback_config:
            final_config.update(final_lookback_config)

        cross_section_configs.append(final_config)

    else:
        # æ— Csç®—å­ï¼šåªæ”¾åœ¨sliceä¸­
        final_config = {
            "name": factor_name,
            "expression": final_expression,
            "trigger": "slice",
            "output": True
        }

        # ä¸ºæœ€ç»ˆè¡¨è¾¾å¼å•ç‹¬åˆ†æå›çœ‹æ—¶é—´
        final_lookback_config = analyze_lookback_requirements(final_expression)
        if final_lookback_config:
            final_config.update(final_lookback_config)

        slice_configs.append(final_config)

    # ç¬¬äº”æ­¥ï¼šæ„å»ºæœ€ç»ˆJSON
    factor_json = {
        "default": default_config,
        "slice": slice_configs
    }

    if cross_section_configs:
        factor_json["cross_section"] = cross_section_configs

    factor_json_path = os.path.join(temp_dir, "factor_config.json")
    with open(factor_json_path, 'w', encoding='utf-8') as f:
        json.dump(factor_json, f, indent=2, ensure_ascii=False)

    # ç”Ÿæˆè¾“å‡ºå› å­åç§°æ–‡ä»¶ï¼ˆåªåŒ…å«output=trueçš„å› å­ï¼‰
    output_names = []

    # æ”¶é›†sliceä¸­output=trueçš„å› å­
    for config in slice_configs:
        if config.get('output', False):
            output_names.append(config['name'])

    # æ”¶é›†cross_sectionä¸­output=trueçš„å› å­
    for config in cross_section_configs:
        if config.get('output', False):
            output_names.append(config['name'])

    output_names_path = os.path.join(temp_dir, "factor_names.txt")
    with open(output_names_path, 'w', encoding='utf-8') as f:
        for name in output_names:
            f.write(f"{name}\n")

    # è¾“å‡ºæ¨¡å—åç§°
    output_module_name = f"set_{factor_name.split('_')[-1]}"

    return factor_json_path, output_names_path, output_module_name


def compute_factor_values_with_lorentz(parsed_expr: Dict[str, str]) -> Tuple[np.ndarray, pd.DatetimeIndex, pd.Index]:
    """
    ä½¿ç”¨Lorentzè®¡ç®—å› å­å€¼

    Args:
        parsed_expr: è§£æåçš„è¡¨è¾¾å¼ä¿¡æ¯

    Returns:
        values: (n_days, n_stocks) çš„å› å­å€¼æ•°ç»„
        dates: æ—¥æœŸç´¢å¼•
        symbols: è‚¡ç¥¨ä»£ç ç´¢å¼•
    """
    config = LorentzConfig()
    executor = LorentzExecutor(config)
    parser = LorentzResultParser(config)

    factor_name = parsed_expr["factor_name"]
    expr_str = parsed_expr["expression"]

    print(f"ğŸ”§ Lorentz Configuration for: {expr_str}", file=sys.stderr)
    print(f"   Factor name: {factor_name}", file=sys.stderr)

    # åˆ›å»ºdebugç›®å½•ç”¨äºä¿å­˜é…ç½®æ–‡ä»¶
    debug_dir = os.path.join(os.getcwd(), 'lorentz_debug')
    os.makedirs(debug_dir, exist_ok=True)
    print(f"DEBUG: Created lorentz_debug directory at: {debug_dir}", file=sys.stderr)

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        # ç”Ÿæˆé…ç½®æ–‡ä»¶
        factor_json_path, output_names_path, output_module_name = generate_lorentz_config_files(
            parsed_expr, temp_dir
        )

        # å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°debugç›®å½•ä¾›æŸ¥çœ‹
        import shutil
        debug_factor_json = os.path.join(debug_dir, 'factor_config.json')
        debug_output_names = os.path.join(debug_dir, 'factor_names.txt')
        shutil.copy2(factor_json_path, debug_factor_json)
        shutil.copy2(output_names_path, debug_output_names)
        print(f"DEBUG: Saved config files to debug directory", file=sys.stderr)

        # ===== æ‰“å°é…ç½®æ–‡ä»¶å†…å®¹ =====
        print(f"\nğŸ“‹ Lorentz Configuration Files for: {expr_str}", file=sys.stderr)

        # æ‰“å° factor_config.json
        print(f"\nğŸ”§ factor_config.json:", file=sys.stderr)
        print("-" * 60, file=sys.stderr)
        try:
            with open(factor_json_path, 'r', encoding='utf-8') as f:
                json_content = f.read()
                print(json_content, file=sys.stderr)
        except Exception as e:
            print(f"âŒ Failed to read factor_config.json: {e}", file=sys.stderr)
        print("-" * 60, file=sys.stderr)

        # æ‰“å° factor_names.txt
        print(f"\nğŸ“ factor_names.txt:", file=sys.stderr)
        print("-" * 60, file=sys.stderr)
        try:
            with open(output_names_path, 'r', encoding='utf-8') as f:
                txt_content = f.read()
                print(txt_content, file=sys.stderr)
        except Exception as e:
            print(f"âŒ Failed to read factor_names.txt: {e}", file=sys.stderr)
        print("-" * 60, file=sys.stderr)

        # æ‰“å° Lorentz ç¨‹åºä¿¡æ¯
        print(f"\nğŸ­ Lorentz Program Information:", file=sys.stderr)
        print(f"   Executable: {config.lorentz_executable}", file=sys.stderr)
        print(f"   Thread num: {config.thread_num}", file=sys.stderr)
        print(f"   Data root: {config.data_root_dir}", file=sys.stderr)
        print(f"   Output root: {config.output_factor_root_dir}", file=sys.stderr)
        print(f"   Start date: {config.start_date}", file=sys.stderr)
        print(f"   End date: {config.end_date}", file=sys.stderr)
        print(f"   Output module: {output_module_name}", file=sys.stderr)

        # è§£ææ—¥æœŸèŒƒå›´
        start_date = datetime.strptime(config.start_date, '%Y%m%d')
        end_date = datetime.strptime(config.end_date, '%Y%m%d')

        all_results = []

        # ä¸ºæ¯ä¸ªæ—¥æœŸæ‰§è¡Œè®¡ç®—
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime('%Y%m%d')

            try:
                # æ‰§è¡ŒLorentzè®¡ç®—
                success, error_msg = executor.execute_for_date(
                    date_str, factor_json_path, output_names_path, output_module_name
                )

                if success:
                    # è§£æç»“æœ
                    result_df = parser.parse_factor_output(date_str, factor_name)
                    if result_df is not None:
                        all_results.append(result_df)
                        logger.info(f"Successfully computed factor for {date_str}")
                    else:
                        logger.warning(f"Failed to parse results for {date_str}")
                else:
                    logger.error(f"Failed to compute factor for {date_str}: {error_msg}")

            except Exception as e:
                logger.error(f"Exception during Lorentz computation for {date_str}: {e}")
                import traceback
                traceback.print_exc()

            # ç§»åŠ¨åˆ°ä¸‹ä¸€å¤©
            current_date += timedelta(days=1)

        if not all_results:
            print(f"ERROR: No factor values were successfully computed for expression {parsed_expr['expression']}", file=sys.stderr)
            raise ValueError("No factor values were successfully computed")

        # åˆå¹¶æ‰€æœ‰æ—¥æœŸçš„ç»“æœ
        combined_df = pd.concat(all_results, ignore_index=True)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„æ ¼å¼
        # é€è§†è¡¨ï¼šè¡Œ=æ—¥æœŸï¼Œåˆ—=è‚¡ç¥¨ä»£ç ï¼Œå€¼=å› å­å€¼
        pivot_df = combined_df.pivot(index='date', columns='symbol', values=factor_name)

        # å¡«å……ç¼ºå¤±å€¼
        pivot_df = pivot_df.fillna(0.0)

        values = pivot_df.values
        dates = pivot_df.index
        symbols = pivot_df.columns

        logger.info(f"Computed factor values: {values.shape}")

        return values, dates, symbols


def compute_batch_factor_values(parsed_exprs: List[Dict], data_source=None) -> Dict[str, Tuple[np.ndarray, pd.DatetimeIndex, pd.Index]]:
    """
    æ‰¹é‡è®¡ç®—å¤šä¸ªå› å­å€¼çš„ä¸»å‡½æ•°

    Args:
        parsed_exprs: è§£æåçš„è¡¨è¾¾å¼åˆ—è¡¨
        data_source: æ•°æ®æºï¼ˆå¯é€‰ï¼‰

    Returns:
        å­—å…¸ï¼šè¡¨è¾¾å¼åç§° -> (values, dates, symbols)
    """
    try:
        if len(parsed_exprs) == 1:
            # å•è¡¨è¾¾å¼ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
            values, dates, symbols = compute_factor_values_with_lorentz(parsed_exprs[0])
            return {parsed_exprs[0]["factor_name"]: (values, dates, symbols)}

        # æ‰¹é‡è®¡ç®—å¤šä¸ªè¡¨è¾¾å¼
        return compute_batch_factor_values_with_lorentz(parsed_exprs)

    except Exception as e:
        # ===== æ‰“å°è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯å¹¶ç»ˆæ­¢ç¨‹åº =====
        print("\n" + "="*80, file=sys.stderr)
        print("ğŸš¨ LORENTZ COMPUTATION FAILED - TERMINATING PROGRAM", file=sys.stderr)
        print("="*80, file=sys.stderr)

        # æ‰“å°è¿™ä¸€æ‰¹çš„æ‰€æœ‰è¡¨è¾¾å¼ä¿¡æ¯
        print(f"\nğŸ“‹ Batch contained {len(parsed_exprs)} expressions:", file=sys.stderr)
        for i, parsed_expr in enumerate(parsed_exprs, 1):
            expr_str = parsed_expr['expression']
            factor_name = parsed_expr['factor_name']
            print(f"  {i}. Factor: {factor_name}", file=sys.stderr)
            print(f"     Expression: {expr_str}", file=sys.stderr)

        # å°è¯•ç”Ÿæˆé…ç½®æ–‡ä»¶å¹¶æ˜¾ç¤ºå†…å®¹
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                if len(parsed_exprs) == 1:
                    factor_json_path, output_names_path, output_module_name = generate_lorentz_config_files(
                        parsed_exprs[0], temp_dir
                    )
                else:
                    factor_json_path, output_names_path, output_module_name = generate_batch_lorentz_config_files(
                        parsed_exprs, temp_dir
                    )

                print(f"\nğŸ“„ Configuration files generated in: {temp_dir}", file=sys.stderr)

                # æ˜¾ç¤ºfactor_config.jsonå†…å®¹
                print(f"\nğŸ“‹ factor_config.json content:", file=sys.stderr)
                print("-" * 40, file=sys.stderr)
                with open(factor_json_path, 'r', encoding='utf-8') as f:
                    json_content = f.read()
                    print(json_content, file=sys.stderr)
                print("-" * 40, file=sys.stderr)

                # æ˜¾ç¤ºfactor_names.txtå†…å®¹
                print(f"\nğŸ“‹ factor_names.txt content:", file=sys.stderr)
                print("-" * 40, file=sys.stderr)
                with open(output_names_path, 'r', encoding='utf-8') as f:
                    txt_content = f.read()
                    print(txt_content, file=sys.stderr)
                print("-" * 40, file=sys.stderr)

                # æ˜¾ç¤ºlorentz_config.cfgå†…å®¹ï¼ˆæ¨¡æ‹Ÿç”Ÿæˆï¼‰
                print(f"\nğŸ“‹ lorentz_config.cfg content (example for first date):", file=sys.stderr)
                print("-" * 40, file=sys.stderr)
                config = LorentzConfig()
                cfg_content = f"""DATE=20240101
INTERVAL_JSON={config.interval_json}

[BasicFields]
DATA_ROOT_DIR={config.data_root_dir}
LOAD_PREV_DAYS=1
THREAD_NUM={config.thread_num}
AUTO_PROD_CO_DEPENDENCY=TRUE
DAILY_DATA_DIR={config.data_root_dir}

[ComputeGraph]
THREAD_NUM={config.thread_num}
FACTOR_JSON={factor_json_path}
OUTPUT_MODULE_NAME={output_module_name}
OUTPUTS_CONFIG_FILES={output_names_path}
EMABLE_OUTPUT_CSV=TRUE
CSV_FLOAT_PRECISION=6
OUTPUT_FACTOR_ROOT_DIR={config.output_factor_root_dir}
OUTPUT_ABNORMAL_ROOT_DIR={config.output_abnormal_root_dir}"""
                print(cfg_content, file=sys.stderr)
                print("-" * 40, file=sys.stderr)

        except Exception as config_error:
            print(f"âŒ Failed to generate/show config files: {config_error}", file=sys.stderr)

        # æ˜¾ç¤ºåŸå§‹é”™è¯¯
        print(f"\nğŸ’¥ Original error: {str(e)}", file=sys.stderr)
        print("\n" + "="*80, file=sys.stderr)

        # ç»ˆæ­¢ç¨‹åº
        sys.exit(1)


def compute_batch_factor_values_with_lorentz(parsed_exprs: List[Dict]) -> Dict[str, Tuple[np.ndarray, pd.DatetimeIndex, pd.Index]]:
    """
    ä½¿ç”¨Lorentzæ‰¹é‡è®¡ç®—å¤šä¸ªå› å­å€¼
    """
    config = LorentzConfig()
    executor = LorentzExecutor(config)
    parser = LorentzResultParser(config)

    # åˆ›å»ºdebugç›®å½•
    debug_dir = os.path.join(os.getcwd(), 'lorentz_debug')
    os.makedirs(debug_dir, exist_ok=True)
    print(f"DEBUG: Created lorentz_debug directory at: {debug_dir}", file=sys.stderr)

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        # ä¸ºæ‰¹é‡è¡¨è¾¾å¼ç”Ÿæˆé…ç½®æ–‡ä»¶
        factor_json_path, output_names_path, output_module_name = generate_batch_lorentz_config_files(
            parsed_exprs, temp_dir
        )

        # å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°debugç›®å½•
        import shutil
        debug_factor_json = os.path.join(debug_dir, 'batch_factor_config.json')
        debug_output_names = os.path.join(debug_dir, 'batch_factor_names.txt')
        shutil.copy2(factor_json_path, debug_factor_json)
        shutil.copy2(output_names_path, debug_output_names)
        print(f"DEBUG: Saved batch config files to debug directory", file=sys.stderr)

        # è§£ææ—¥æœŸèŒƒå›´
        start_date = datetime.strptime(config.start_date, '%Y%m%d')
        end_date = datetime.strptime(config.end_date, '%Y%m%d')

        all_results = {}

        # è®¡ç®—LOAD_PREV_DAYSï¼ˆä¸generate_batch_lorentz_config_filesä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
        max_prev_days = 1  # æœ€å°å€¼
        for parsed_expr in parsed_exprs:
            expr_str = parsed_expr["expression"]
            parsed_result = parse_expression_with_intermediates(convert_field_references(expr_str))
            all_subexpressions = []
            for intermediate in parsed_result['slice_intermediates']:
                all_subexpressions.append(intermediate['expression'])
            for intermediate in parsed_result['cross_section_intermediates']:
                all_subexpressions.append(intermediate['expression'])
            all_subexpressions.append(parsed_result['final_expression'])

            for sub_expr in all_subexpressions:
                lookback_config = analyze_lookback_requirements(sub_expr)
                if 'rolling_prev_days' in lookback_config:
                    max_prev_days = max(max_prev_days, lookback_config['rolling_prev_days'])

        load_prev_days = max_prev_days

        # ä¸ºæ¯ä¸ªæ—¥æœŸæ‰§è¡Œæ‰¹é‡è®¡ç®—
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime('%Y%m%d')

            try:
                # æ‰§è¡Œæ‰¹é‡Lorentzè®¡ç®—
                success, error_msg = executor.execute_for_date(
                    date_str, factor_json_path, output_names_path, output_module_name, load_prev_days
                )

                if success:
                    # è§£ææ‰¹é‡ç»“æœ
                    batch_results = parser.parse_batch_factor_output(date_str, [expr["factor_name"] for expr in parsed_exprs])
                    if batch_results:
                        for factor_name, result_df in batch_results.items():
                            if factor_name not in all_results:
                                all_results[factor_name] = []
                            all_results[factor_name].append(result_df)
                        logger.info(f"Successfully computed batch factors for {date_str}")
                    else:
                        logger.warning(f"Failed to parse batch results for {date_str}")
                else:
                    # Lorentz æ‰§è¡Œå¤±è´¥ï¼Œæ‰“å°è¯¦ç»†è¯Šæ–­ä¿¡æ¯å¹¶ç»ˆæ­¢ç¨‹åº
                    print(f"\n" + "="*100, file=sys.stderr)
                    print(f"ğŸš¨ LORENTZ EXECUTION FAILED FOR {date_str} - TERMINATING PROGRAM", file=sys.stderr)
                    print("="*100, file=sys.stderr)

                    # æ‰“å°å¤±è´¥çš„åŸºæœ¬ä¿¡æ¯
                    print(f"\nâŒ Lorentz execution failed with error: {error_msg}", file=sys.stderr)
                    print(f"ğŸ“… Date: {date_str}", file=sys.stderr)
                    print(f"ğŸ”¢ Load prev days: {load_prev_days}", file=sys.stderr)

                    # æ˜¾ç¤ºé…ç½®æ–‡ä»¶å†…å®¹
                    print(f"\nğŸ“‹ Configuration files content:", file=sys.stderr)

                    # æ˜¾ç¤ºfactor_config.json
                    print(f"\nğŸ”§ factor_config.json:", file=sys.stderr)
                    print("-" * 60, file=sys.stderr)
                    try:
                        with open(factor_json_path, 'r', encoding='utf-8') as f:
                            json_content = f.read()
                            print(json_content, file=sys.stderr)
                    except Exception as e:
                        print(f"âŒ Failed to read factor_config.json: {e}", file=sys.stderr)
                    print("-" * 60, file=sys.stderr)

                    # æ˜¾ç¤ºfactor_names.txt
                    print(f"\nğŸ“ factor_names.txt:", file=sys.stderr)
                    print("-" * 60, file=sys.stderr)
                    try:
                        with open(output_names_path, 'r', encoding='utf-8') as f:
                            txt_content = f.read()
                            print(txt_content, file=sys.stderr)
                    except Exception as e:
                        print(f"âŒ Failed to read factor_names.txt: {e}", file=sys.stderr)
                    print("-" * 60, file=sys.stderr)

                    # æ˜¾ç¤ºlorentz_config.cfgï¼ˆä»debugç›®å½•è¯»å–ï¼‰
                    debug_dir = os.path.join(os.getcwd(), 'lorentz_debug')
                    cfg_file_path = os.path.join(debug_dir, f'lorentz_config_{date_str}.cfg')
                    print(f"\nâš™ï¸ lorentz_config.cfg ({date_str}):", file=sys.stderr)
                    print("-" * 60, file=sys.stderr)
                    try:
                        with open(cfg_file_path, 'r', encoding='utf-8') as f:
                            cfg_content = f.read()
                            print(cfg_content, file=sys.stderr)
                    except Exception as e:
                        print(f"âŒ Failed to read lorentz_config.cfg: {e}", file=sys.stderr)
                    print("-" * 60, file=sys.stderr)

                    # æ˜¾ç¤ºLorentzç¨‹åºä¿¡æ¯
                    config = LorentzConfig()
                    print(f"\nğŸ­ Lorentz Program Information:", file=sys.stderr)
                    print(f"   Executable: {config.lorentz_executable}", file=sys.stderr)
                    print(f"   Thread num: {config.thread_num}", file=sys.stderr)
                    print(f"   Data root: {config.data_root_dir}", file=sys.stderr)
                    print(f"   Output root: {config.output_factor_root_dir}", file=sys.stderr)

                    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    print(f"\nğŸ“ Input Files Check:", file=sys.stderr)
                    files_to_check = [
                        ('Interval JSON', config.interval_json),
                        ('Data Root', config.data_root_dir),
                        ('Factor JSON', factor_json_path),
                        ('Output Names', output_names_path),
                    ]

                    for name, path in files_to_check:
                        exists = os.path.exists(path)
                        status = "âœ… EXISTS" if exists else "âŒ MISSING"
                        print(f"   {name}: {path} - {status}", file=sys.stderr)

                        if not exists and name in ['Interval JSON', 'Data Root']:
                            print(f"      âš ï¸  This is a critical file for Lorentz execution!", file=sys.stderr)

                    # æ˜¾ç¤ºé¢„æœŸçš„è¾“å‡ºç›®å½•
                    expected_output_dir = os.path.join(
                        config.output_factor_root_dir,
                        "AutoML",
                        date_str[:4],  # å¹´ä»½
                        date_str      # å®Œæ•´æ—¥æœŸ
                    )
                    print(f"\nğŸ“¤ Expected Output Directory: {expected_output_dir}", file=sys.stderr)
                    if os.path.exists(expected_output_dir):
                        print(f"   Status: âœ… EXISTS", file=sys.stderr)
                        # åˆ—å‡ºç›®å½•å†…å®¹
                        try:
                            contents = os.listdir(expected_output_dir)
                            csv_files = [f for f in contents if f.endswith('.csv')]
                            print(f"   CSV files found: {len(csv_files)}", file=sys.stderr)
                            if csv_files:
                                print(f"   Sample files: {csv_files[:3]}", file=sys.stderr)
                        except Exception as e:
                            print(f"   Error listing directory: {e}", file=sys.stderr)
                    else:
                        print(f"   Status: âŒ DOES NOT EXIST", file=sys.stderr)

                    print(f"\nğŸ’¥ TERMINATING PROGRAM DUE TO LORENTZ FAILURE", file=sys.stderr)
                    print("="*100, file=sys.stderr)

                    # ç»ˆæ­¢ç¨‹åº
                    sys.exit(1)

            except Exception as e:
                # éLorentzæ‰§è¡Œå¼‚å¸¸ï¼Œæ‰“å°å¹¶ç»§ç»­ï¼ˆæˆ–ç»ˆæ­¢ï¼Œæ ¹æ®ä¸¥é‡ç¨‹åº¦ï¼‰
                logger.error(f"Exception during batch Lorentz computation for {date_str}: {e}")
                import traceback
                traceback.print_exc()

                # å¯¹äºä¸¥é‡å¼‚å¸¸ï¼Œä¹Ÿç»ˆæ­¢ç¨‹åº
                print(f"\nğŸ’¥ CRITICAL EXCEPTION DURING FACTOR COMPUTATION - TERMINATING", file=sys.stderr)
                sys.exit(1)

            current_date += timedelta(days=1)

        if not all_results:
            raise ValueError("No factor values were successfully computed in batch mode")

        # åˆå¹¶ç»“æœå¹¶è¿”å›
        final_results = {}
        for factor_name, result_dfs in all_results.items():
            if result_dfs:
                combined_df = pd.concat(result_dfs, ignore_index=True)
                # è½¬æ¢ä¸ºnumpyæ•°ç»„æ ¼å¼
                pivot_df = combined_df.pivot(index='date', columns='symbol', values=factor_name)
                pivot_df = pivot_df.fillna(0.0)
                values = pivot_df.values
                dates = pivot_df.index
                symbols = pivot_df.columns
                final_results[factor_name] = (values, dates, symbols)

        return final_results


def generate_batch_lorentz_config_files(parsed_exprs: List[Dict], temp_dir: str) -> Tuple[str, str, str]:
    """
    ä¸ºæ‰¹é‡è¡¨è¾¾å¼ç”ŸæˆLorentzé…ç½®æ–‡ä»¶
    """
    # è®¡ç®—LOAD_PREV_DAYS
    max_prev_days = 1  # æœ€å°å€¼
    has_prev_days = False

    for parsed_expr in parsed_exprs:
        expr_str = parsed_expr["expression"]

        # è§£æè¡¨è¾¾å¼ï¼Œæå–æ‰€æœ‰å­è¡¨è¾¾å¼
        parsed_result = parse_expression_with_intermediates(expr_str)
        all_subexpressions = []

        # æ”¶é›†æ‰€æœ‰å­è¡¨è¾¾å¼
        for intermediate in parsed_result['slice_intermediates']:
            all_subexpressions.append(intermediate['expression'])
        for intermediate in parsed_result['cross_section_intermediates']:
            all_subexpressions.append(intermediate['expression'])
        all_subexpressions.append(parsed_result['final_expression'])

        # æ£€æŸ¥æ¯ä¸ªå­è¡¨è¾¾å¼çš„rolling_prev_days
        for sub_expr in all_subexpressions:
            lookback_config = analyze_lookback_requirements(sub_expr)
            if 'rolling_prev_days' in lookback_config:
                max_prev_days = max(max_prev_days, lookback_config['rolling_prev_days'])
                has_prev_days = True

    # å¦‚æœæ²¡æœ‰rolling_prev_daysï¼Œä½¿ç”¨é»˜è®¤å€¼1
    load_prev_days = max_prev_days if has_prev_days else 1

    # æ„å»ºæ‰¹é‡é…ç½®
    default_config = {
        "slice": {
            "trigger": "slice",
            "output": True
        }
    }

    slice_configs = []

    # ä¸ºæ¯ä¸ªè¡¨è¾¾å¼åˆ›å»ºé…ç½®ï¼Œæ‰€æœ‰outputéƒ½è®¾ä¸ºtrue
    for parsed_expr in parsed_exprs:
        factor_name = parsed_expr["factor_name"]
        expression = parsed_expr["expression"]

        # è½¬æ¢å­—æ®µå¼•ç”¨
        converted_expression = convert_field_references(expression)

        # è§£æè¡¨è¾¾å¼
        parsed_result = parse_expression_with_intermediates(converted_expression)

        # ä¸ºsliceä¸­é—´å˜é‡åˆ›å»ºé…ç½®
        for intermediate in parsed_result['slice_intermediates']:
            slice_configs.append(intermediate)

        # ä¸ºæœ€ç»ˆè¡¨è¾¾å¼åˆ›å»ºé…ç½®
        final_config = {
            "name": factor_name,
            "expression": parsed_result['final_expression'],
            "trigger": "slice",  # æ‰¹é‡æ¨¡å¼éƒ½æ”¾åœ¨slice
            "output": True  # æ‰€æœ‰è¡¨è¾¾å¼outputéƒ½ä¸ºtrue
        }

        # æ·»åŠ lookbacké…ç½®
        final_lookback = analyze_lookback_requirements(parsed_result['final_expression'])
        if final_lookback:
            final_config.update(final_lookback)

        slice_configs.append(final_config)

    # æ„å»ºæœ€ç»ˆJSON
    factor_json = {
        "default": default_config,
        "slice": slice_configs
    }

    factor_json_path = os.path.join(temp_dir, "batch_factor_config.json")
    with open(factor_json_path, 'w', encoding='utf-8') as f:
        json.dump(factor_json, f, indent=2, ensure_ascii=False)

    # ç”Ÿæˆè¾“å‡ºå› å­åç§°æ–‡ä»¶ï¼ˆæ‰€æœ‰å› å­ï¼‰
    output_names = [expr["factor_name"] for expr in parsed_exprs]
    output_names_path = os.path.join(temp_dir, "batch_factor_names.txt")
    with open(output_names_path, 'w', encoding='utf-8') as f:
        for name in output_names:
            f.write(f"{name}\n")

    # è¾“å‡ºæ¨¡å—åç§°
    output_module_name = f"batch_set_{hash(str(output_names)) % 10000}"

    return factor_json_path, output_names_path, output_module_name


def compute_factor_values(parsed_expr: dict, data_source=None) -> Tuple[np.ndarray, pd.DatetimeIndex, pd.Index]:
    """
    è®¡ç®—å•ä¸ªå› å­å€¼çš„ä¸»å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    """
    return compute_batch_factor_values([parsed_expr], data_source)[parsed_expr["factor_name"]]


def main():
    """
    ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è®¡ç®—å› å­å€¼
    """
    if len(sys.argv) != 2:
        print("Usage: python external_compute.py <alpha_expression>", file=sys.stderr)
        sys.exit(1)

    expr_str = sys.argv[1]

    try:
        # è§£æè¡¨è¾¾å¼
        parsed_expr = parse_alpha_expression(expr_str)
        print(f"Parsed expression: {expr_str}", file=sys.stderr)

        # è®¡ç®—å› å­å€¼
        values, dates, symbols = compute_factor_values(parsed_expr)

        # è¾“å‡ºCSVæ ¼å¼ç»“æœ
        print("date,symbol,value")

        # éå†æ‰€æœ‰æ—¥æœŸå’Œè‚¡ç¥¨
        for i, date in enumerate(dates):
            for j, symbol in enumerate(symbols):
                value = values[i, j]
                if not np.isnan(value):  # åªè¾“å‡ºéNaNå€¼
                    print(f"{date.strftime('%Y-%m-%d')},{symbol},{value:.6f}")

    except Exception as e:
        print(f"Fatal error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()

